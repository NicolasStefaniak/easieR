---
title: "Formation easieR à Bruxelles"
author: "Nicolas Stefaniak"
date: "25 octobre 2021"
output: 
 html_document:
               toc: true
               toc_float: true
               toc_depth: 5
logo: URCA.png
---

```{r, eval = T, echo=F}

.multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
} 
```

# 1. Introduction 

## 1.1. Pourquoi R ?
![](Rlogo-1.png)

- Gratuit
- Compatible avec tous les systèmes d’exploitation 
- Offre des opportunités d’emploi 
- Offre plus de fonctionnalités que les logiciels payants : 150 fois plus de fonctions que SAS (Muenchen, 2015)
- Est collaboratif 
- Est ouvert
- Est reproductible/transparent 

## 1.2. Pourquoi easieR ?

- Être capable d’être autonome rapidement 
- Harmoniser les sorties de résultats 
- Être le moins possible confronté aux messages d’erreur 
- Se familiariser avec l’environnement R
- Forcer à prendre/à comprendre ses décisions statistiques 
- Forcer à avoir une bonne hygiène dans l’analyse des données 
- Apprendre progressivement à l’utilisation des lignes de commandes. 


### 1.2.1. Se familiariser avec l'environnement 

Lorsque vous lancez R, la fenêtre qui apparaît est la console (Figure 1). 

![](console.R.png)
<center><small>Figure 1. Console R </small></center>

Il est possible de taper des lignes de commandes directement dans la console. Cependant, cette manière de faire est à proscrire pour plusieurs raisons : 

- la console ne permet pas de modifier aisément les erreurs de frappes ; 
- la console ne permet pas de garder aisément une trace des opérations qui ont été faites ; 
- la console ne permet pas de commenter les lignes de commandes de sorte à pouvoir s'y référer ultérieurement. 

Une manière bien plus adaptée d'utiliser R est d'utiliser un script. Pour cela, il faut suivre les instructions en Figure 2.

![](script.png)
<small>Figure 2. Créer un script R </small>

__Note : le script s'ouvre automatiquement pour les utilisateurs de RStudio__

Une fenêtre blanche apparaît et c'est dans cette fenêtre que vous allez taper les lignes de commandes. Vous pourrez également les exécuter à partir du script : 

- pour les utilisateurs windows, en utilisant le raccourci : ctrl+R sur la/les ligne(s) de commande à exécuter ;

- pour les utilisateurs mac, en utilisant le raccourci : pomme+entrée sur la/les ligne(s) de commande à exécuter. 

Dans ce script, outre les lignes de commandes que vous taperez et exécuterez, vous allez pouvoir commenter les opérations que vous réaliser. Tout les lignes débutant par un dièse (#) sera considérée comme du commentaire. 

Une bonne pratique est de garder une trace de tous les opérations que nous faisons en écrivant toutes les lignes de commandes dans le script R. 


### 1.2.2. Se forcer à prendre des décisions statistiques 

La plupart du temps, les personnes qui réalisent des analyses statistiques utilisent un test sans connaître les tenants et aboutissants du test. Ils l'utilisent parce qu'on leur a dit qu'il fallait utiliser ce test ou parce que ce test a été utilisé dans un article. 

Adopter ce type de logique atteint rapidement ses limites quand on est confronté à des experts qui ont des connaissances en statistiques. Il faut donc pouvoir justifier ses choix. 

L'intérêt de easieR est que, pour une analyse donnée, on ne peut pas ignorer les options : comme les boîtes de dialogue apparaissent de manière successives, il faut prendre une décision pour chacune des boîtes de dialogue. 


### 1.2.3. Se forcer à avoir une bonne hygiène dans l'analyse de données. 

Bien qu'il soit difficile d'estimer avec précision l'ampleur du problème, on peut soupçonner qu'une des causes de la non reproductibilité des études est la présence d'erreurs dans le jeu de données. 

Ces erreurs peuvent être minimisées de trois manières : 
1) lors de l'importation, en s'assurant que le fichier importer a le bon nombre de variables et d'observations. 
2) lors de l'importation, en s'assurant que la nature des variables correspond à la nature qu'ils sont censés avoir. 
3) en vérifiant que les statistiques descriptives sont compatibles avec les variables à analyser. Par exemple, si vous avez une échelle de Likert qui va de 1 à 7, vous ne devez pas avoir de 0 ou de 8. Vérifier le minimum et le maximum est rapide à réaliser, est une stratégie efficace pour identifier des valeurs anormales, et pourtant il s'agit d'un réflexe souvent ignoré. Dans le cas d'easieR, les statistiques descriptives sont systématiques fournies avec l'analyse à réaliser. 

Outre les erreurs dans les jeux de données, il n'est pas rare que les personnes moins à l'aise en statistiques ne connaissent pas les conditions d'application, ou si elles connaissent les conditions d'application, ne connaissent pas une alternative au test souhait quand les conditions d'application ne sont pas respectées. Avec easieR, les conditions d'application ne sont pas des options, elles font partie de la sortie de résultat et des alternatives au test utilisé en première intention sont proposées. 

Pour terminer, easieR permet aussi d'avoir comme logique d'identifier/supprimer les valeurs influentes. S'il y a débat sur le fait de supprimer ou non les valeurs influentes, il faut au minimum les identifier et les discuter. 


# 2. Débuter avec R
## 2.1. Installer des packages 

R est composé d’un ensemble de magasins, appelés packages. Il y en a en tout
plus de 18 000. Cependant, nous avons besoin que d’une minorité de ces magasins. 

Les packages dont vous aurez besoin sont fournis dans les chapitres consacrés. Néanmoins, il est important de connaître la logique générale de l'installation d'un package. Tout d'abord, il est nécessaire de choisir un CRAN. Il est d'usage de choisir un CRAN proche de l'endroit où on habite. Le CRAN que je choisis habituellement est Lyon2. Pour choisir Lyon2 comme CRAN, on tape  : 


```{r}
# ceci est un commentaire 
options(repos = "https://mirror.ibcp.fr/pub/CRAN") # cette ligne de commande permet de choisir le répertoire pour le CRAN 
```


Ensuite, on installe un package avec la fonction  <code>install.packages</code>. Nous aurons par exemple besoin du package *psych*. Nous allons donc l'installer de la manière suivante :


```{r eval=FALSE}
install.packages("psych")
```


## 2.2. Installer easieR

easieR est un package qui peut être utilisé en boîte de dialogue (GUI) ou en ligne de commande. Ce document expliquera comment l'utiliser d'une manière ou d'une autre. Cependant, pour le moment, easieR n'est pas un package disponible sur le CRAN. Il est donc nécessaire de passer par quelques étapes intermédiaires. Pour s'assurer d'une installation correcte de easieR, il faut suivre **4 étapes**.  

**Etape 1** : tout d'abord, il faut  ouvrir R et installer le package *devtools* de la manière suivante. 

```{r eval=FALSE}
install.packages("devtools",dependencies = TRUE, repos = "https://mirror.ibcp.fr/pub/CRAN")
```


Normalement, le package « devtools » est à présent installé. Il faut le charger. Pour charger un package, on utilise la fonction <code>library</code>. Une fois que le package est chargé, vous pouvez l’utiliser pendant toute la durée de la session sans devoir le charger à nouveau. Donc, pour charger le package *devtools*, il faut taper :

```{r}
library(devtools)
```


Si un message d’erreur survient ici (« erreur » apparaît dans la console), c’est que vous n’avez pas réalisé correctement les opérations qui précèdent. Si le package est installé correctement, R va indiquer :

```{r echo=F}
print("##Le chargement a nécessité le package : devtools")
```

Il se peut aussi que rien ne soit affiché. Ce n’est pas grave.

**Etape 2**: l'installation de easieR 

Vous pouvez installer easieR à présent grâce à la ligne de commande :

```{r ,eval=FALSE}
install_github("NicolasStefaniak/easieR", type="binary")
```

Vous savez que easieR est installé en chargeant le package à l'aide de la ligne de commande suivante :

```{r eval=FALSE}
library(easieR)
```



La date de la dernière mise à jour est également indiquée (Last update). Il est bon de vérifier sur github dans l'onglet "nouvelles fonctionnalités et bugs" si des versions plus récentes sont proposées. 

**Etape 3**: Vérifier l'installation des packages

Pour pouvoir profiter de l’ensemble des fonctionnalités de easieR, il est nécessaire de vérifier que tous les packages soient bien installés. Cela peut être réalisé grâce à la fonction :

```{r eval=FALSE }
ez.install()
```

A cette étape, vous saurez que tous les packages sont correctement installés si dans la sortie de résultats, vous avez "packages.mal.installes : character(0)". 



**Etape 4**: l'installation de Pandoc

La dernière étape consiste à installer pandoc. Ce logiciel complémentaire vous permet d'obtenir les sorties html pour les résultats. 

Pour pandoc, il faut se rendre sur le site de pandoc (https://github.com/jgm/pandoc/releases/)  et de l’installer manuellement. Vous devez d'abord avoir fermé R avant de l'installer. Vous choisissez la version qui correspond à votre ordinateur (le fichier pkg pour les utilisateurs **mac** et le fichier msi pour les utilisateurs **windows**). Après l'avoir téléchargé, vous double-cliquez et vous suivez les consignes d’installation.


Chaque fois que vous fermez R, les packages sont déchargés. Pour pouvoir utiliser easieR, il faudra donc utiliser la fonction <code>library</code> et ensuite, vous pourrez utiliser la fonction easieR :

```{r eval=FALSE }
library(easieR)
```

## 2.3. Mise à jour de easieR 

Dès qu’un bug est identifié, des corrections du package sont réalisées. Par ailleurs, de nouvelles fonctionnalités sont régulièrement ajoutées à easieR. Dès lors, pour pouvoir en profiter, il est utile de pouvoir faire une mise à jour. En d’autres termes, la procédure décrite ci-dessous est utile pour les personnes qui ont déjà installé easieR et qui veulent le mettre à jour. La procédure la plus sûre pour atteindre cet objectif est :

1) d’ouvrir R (si R est déjà ouvert, fermez R au préalable pour vous assurer que des packages ne sont pas chargés)

2) copier coller les lignes de commandes ci-dessous : 


```{r eval=FALSE}
devtools::install_github("nicolasstefaniak/easier", type="binary", dependencies=F)
```

# 3. Débuter avec easieR

Pour commencer à utiliser easieR, nous allons débuter par l'importation de données. L'exemple de travail que nous allons utiliser est le suivant : dans une publicité, Georges Clooney va chercher un café au même moment qu'une jeune femme. Il s'aperçoit que c'est la dernière capsule et la laisse donc à la jeune femme, qui était venue chercher un café pour Jean Dujardin. La question que se pose les chercheur est de savoir comment on interprète cette publicité. Est-ce que Georges Clooney se laisse manipuler par les femmes, est-ce qu'il s'agit d'un gentleman ou est-ce que Jean Dujardin a eu raison de manipuler Clooney pour arriver à ses fins. 

Les données sont disponibles dans le feuille de calcul appelée 'Clooney'.



## 3.1. Importer les données {.tabset .tabset-fade .tabset-pills}

### Avec les boîtes de dialogue


Pour importer des données avec les boîtes de dialogue, il suffit de lancer easieR avec la fonction <code>easieR</code>

```{r eval=FALSE}
easieR()
```


La boîte de dialogue de la Figure 3 apparaît. Il faut choisir "Donnees - (importation, exportation, sauvegarde) et cliquer sur OK. 

![](.\import\import1.png)

<small>Figure 3. choisir "Donnees - (importation, exportation, sauvegarde)".</small> 

De manière assez transparente, pour importer les données, il faut choisir dans la boîte de dialogue de la Figure 4 "importer des donnees"

![](.\import\import2.png)

<small>Figure 4. choisir "importer des donnees".</small> 


easieR permet d'importer des données de 4 types de format : CSV, txt, excel et les fichiers SPSS. En réalité, R est en mesure d'importer d'autres formats de données, mais ces formats représentent les outils utilisés le plus fréquemment utilisés par les psychologues. En l'occurrence, nous travaillerons avec des fichiers excel (voir Figure 5). Il est possible d'accéder directement à cette boîte de dialogue grâce à la fonction <code>import()</code>



![](.\import\import3.png)

<small>Figure 5.Format du fichier de données à importer.</small> 


Une fois le format du fichier décidé, il ne reste plus qu'à choisir le fichier de données (Figure 6). 

![](.\import\import4.png)

<small>Figure 6.Choix du fichier de données, 'Exercices.Bruxelles.xlsx' en l'occurrence.</small> 

La plupart du temps, les chercheurs utilisent la première ligne du fichier de données pour indiquer le nom des variables. Cependant, ce n'est pas toujours le cas. Il est donc nécessaire d'indiquer si  la première ligne correspond effectivement au nom des variables (Figure 7). 


![](.\import\import5.png)
<small>Figure 7.Est-ce que la première ligne correspond aux variables ?.</small> 

De la même manière, quand une valeur est manquante, il y a plusieurs manières d'indiquer les valeurs manquantes. Si la cellule est vide, on peut laisser "NA" en revanche, si une valeur par défaut (comme -9999) est utilisée alors, il faut l'indiquer dans la Figure 8.

![](.\import\import7.png)

<small>Figure 8.Valeurs indiquant que la valeur est manquante. Si la cellule est vide, laissez NA.</small> 


L'étape suivante consistera à choisir la feuille de calcul qui nous intéresse. En l'occurrence, il s'agit de la feuille 'Clooney' (Figure 9). 

![](.\import\import8.png)

<small>Figure 9.Format du fichier de données à importer.</small> 

Et il faut terminer en donnant un nom aux données qui sera utilisé par la suite dans R. Par défaut, c'est le nom de la feuille de calcul (Figure 10). Ce jeu de données doit avoir un nom qui ne contient pas de caractères spéciaux (excepté _ ou .), pas de caractères avec des accents ou des cédilles, et préférez des noms cours (max 20 caractères). 

![](.\import\import9.png)

<small>Figure 10.Nom attribué aux données pour les utiliser ensuite dans R.</small> 

Le jeu de données apparaît à présent dans la console (Figure 11). 

![](.\import\import11.png)
<small>Figure 11.Tableau du jeu de données.</small> 

Ainsi que des informations sur les données. En l'occurrence, il s'agit d'un jeu de données avec 67 observatons, 3 variables qui sont toutes les trois des variables catégorielles (Figure 12)


![](.\import\import12.png)
<small>Figure 12.Structure du jeu de données.</small> 



**Remarques importantes** 

Trois remarques doivent être faites :

- Les explications des boîtes de dialogue sont fournies dans la console quand on utilise easieR, ce qui permet de savoir à quoi elles correspondent même si on ne le sait pas (Figure 13) ;
![](.\import\import10.png)

<small>Figure 13.Information des boîtes de dialogue qui sont affichées dans la console.</small> 

- easieR va changer automatiquement le répertoire de travail pour travailler dans le même répertoire que celui où se trouve le jeu de données ;

- dans la console apparaît un ligne appelée "call". Cette ligne correspond à la ligne de commande (il faut enlever les guillemets au début et à la fin) pour importer le fichier de données (Figure 14).

![](.\import\import13.png)

<small>Figure 14.Ligne de commande qui correspond à l'importation du fichier. Attention, pour l'utiliser, il faut supprimer les guillemets des extrémités.</small> 

### En ligne de commande

On peut également utiliser easieR en ligne de commande. Cette fonction a 8 arguments : 

- file : le nom du fichier 
- dir : le répertoire où se trouve le fichier 
- type : le type de fichier 
- dec : lorsqu'on utilise un fichier csv ou txt, le séparateur de décimale peut être soit un point, soit une virgule. Il faut le préciser. Dans excel, la fonction le détecte automatiquement. 
- sep : dans les fichiers txt et csv, les colonnes peuvent être séparés par différents types de caractères (tabulation, virgule, espace ou point-virgule). Il faut alors indiquer quel est le séparateur de colonnes. Dans excel, il l'identifie par défaut. 
- na.strings : caractères utilisés pour indiquer qu'une valeur est manquante. NA est la valeur par défaut. 
- sheet : pour les fichiers excel, il faut indiquer la feuille de calcul dans laquelle se trouve les données
- name : correspond au nom qu'on veut attribuer au jeu de données dans R. Ce jeu de données doit avoir un nom qui ne contient pas de caractères spéciaux (excepté _ ou .), pas de caractères avec des accents ou des cédilles, et préférez des noms cours (max 20 caractères). 

De manière concrète pour importer le même fichier que celui importé en boîte de dialogue, on utiliser la fonction suivante : 

```{r eval=F}
import(file='Exercices.Bruxelles.xlsx',
       dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
       type='Fichier Excel',
       dec='.',
       sep=';',
       na.strings='NA',
       sheet='Clooney',
       name='Clooney')
```


Le jeu de données apparaît à présent dans la console (Figure 11b). 

![](.\import\import11.png)

<small>Figure 11b.Tableau du jeu de données.</small> 

Ainsi que des informations sur les données. En l'occurrence, il s'agit d'un jeu de données avec 67 observatons, 3 variables qui sont toutes les trois des variables catégorielles (Figure 12b)


![](.\import\import12.png)

<small>Figure 12b.Structure du jeu de données.</small> 

#

Vous avez donc à présent importer le jeu de données présenté dans le Tableau 1.

```{r, echo=F, include=FALSE}
library(easieR)

library(kableExtra)
library(DT)
import(file='Exercices.Bruxelles.xlsx',
       dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
       type='Fichier Excel',
       dec='.',
       sep=';',
       na.strings='NA',
       sheet='Clooney',
       name='Clooney')

import(file='Exercices.Bruxelles.xlsx',
       dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
       type='Fichier Excel',
       dec='.',
       sep=';',
       na.strings='NA',
       sheet='impact_pro',
       name='impact.pro')

import(file='Exercices.Bruxelles.xlsx',
       dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
       type='Fichier Excel',
       dec='.',
       sep=';',
       na.strings='NA',
       sheet='engagement',
       name='engagement')
```

```{r, echo=F}


datatable(Clooney, caption="Tableau 1. Jeu de données 'Clooney'")

```






## 3.2. Exercice {.tabset .tabset-fade .tabset-pills}

### Enoncés 

1) Quelle bonne pratique avez-vous utilisée ? 

2) Importer en utilisant les boîtes de dialogue la feuille "impact_pro". Utiliser comme nom "impact.pro"

3) Importer en utilisant la ligne de commande la feuille "engagement". Appelez-là "engagement". 


### Solutions 


1) Vous avez copier/coller la ligne de commande dans votre script et éventuellement commenter le script pour avoir des explications sous la main

2) vous devez avoir le jeu de données dans le Tableau 2

```{r, echo=F}

str(impact.pro)
datatable(impact.pro, caption="Tableau 2. Jeu de données 'impact_pro'")

```

3) vous devez avoir le jeu de données dans le Tableau 3

```{r, echo=F}

str(engagement)
datatable(engagement, caption="Tableau 3. Jeu de données 'engagement'")

```


# 4. Les $\chi^2$ d'indépendance 

## 4.1. Réaliser un $\chi^2${.tabset .tabset-fade .tabset-pills}   

### Avec les boîtes de dialogue 

Pour réaliser un $\chi^2$, on peut repartir de la fonction easieR. Cependant, à présent, on choisit dans le premier menu "Analyses - Tests d'hypothèse (Figure 15) 


![](.\chi\chi1.png) 

<small>Figure 15.Boîte de dialogue d'accueil de easieR.</small> 

Ensuite, un nouveau menu arrive permettant de choisir l'analyse. Il faut choisir "chi deux" (Figure 16).

![](.\chi\chi2.png)

<small>Figure 16.Choix des chi deux.</small> 

Il existe en réalité plusieurs $\chi^2$. En l'occurrence, nous avons deux variables à groupes indépendants. On doit donc faire un chi² d'indépendance. Si nous avions un tableau $2 \times2$, dont une des variables est en mesure répétée, nous aurions utilisé un test de McNemar. 

**Note** Il est possible d'accéder directement à ce menu en utilisant la fonction <code>chi()</code>


![](.\chi\chi3.png)

<small>Figure 17.Choix du type de chi deux.</small> 

Comme dans la mémoire de R, nous avons plusieurs jeux de données, il faut choisir le jeu de données sur lequel nous voulons réaliser des analyses. S'il n'y en avait qu'un, easieR choisit par défaut le jeu de données disponible.  

![](.\chi\chi4.png)

<small>Figure 17.Choix du jeu de données sur lequel réaliser les analyses.</small> 


Une fois le jeu de données choisi, il faut préciser les variables sur lesquelles l'analyse doit être réalisée. On choisit d'abord la variables qui correspondent aux lignes du tableau (figure 18) et ensuite celle qui correspond aux colonnes (Figure 19). 

**Note** Il est possible de choisir plusieurs variables à chacune des deux étapes. Dans ce cas, easieR fera le $\chi^2$ de tous les variables choisie dans la première boîte avec toutes celles dans la seconde boîte. Un rappel sur le risque de multiplication de l'erreur de première espèce sera également affiché. 

![](.\chi\chi5.png)

<small>Figure 18.Choix de la variable qui correspond aux lignes.</small> 


![](.\chi\chi6.png)

<small>Figure 19.Choix de la variable qui correspond aux colonnes.</small> 

Quand on réaliser un $\chi^2$, les données peuvent être présentées de deux manières. La première manière est celle que nous avons dans notre jeu de données Clooney. Une seconde manière est une présentation synthétique où le tableau est présenté sous la forme d'une combinaire des associations entre les variables catégorielles et une variables avec les effectifs complète le jeu de données. Comme dans le tableau 2 (impact.pro). Lorsque les données sont présentées de manière synthétique de cette manière, il faut pondérer les deux variables précédentes en utilisant une variable effectif. Dans le cas du tableau 2, cette variable est la variable 'Frequence'. En l'occurrence, nous ne devons pas réaliser de pondération avec le jeu de données Clooney (Figure 20). 


![](.\chi\chi7.png)
<small>Figure 20. Lorsque chaque ligne ne correspond pas à un participant mais à un ensemble de participants, il faut pondérer par une variable qui indique les effectifs pour chaque ligne.</small> 

L'étape suivante consiste à déterminer quelles sont les sorties de résultats souhaités pour l'analyse. Il est possible de cocher une ou plusieurs possibilités en appuyant sur la touche "contrôle". En l'occurrence, on souhaite uniquement les résultats d'un $\chi^2$ classique (Figure 21). 

![](.\chi\chi8.png)

<small>Figure 21. Choix des analyses souhaitées. En l'occurrence, uniquement le $\chi^2$ classique.</small>

La dernière boîte de dialogue permet de sauvegarder les résultats en word. Cette option n'est pas compatible avec les utilisateurs macOS. Par défaut, nous ne le feront pas (parce qu'il existe une autre option qui sera bien plus efficace pour sauvegarder les analyses que nous avons réalisées).

![](.\chi\chi9.png)



### Avec les lignes de commande 

On peut obtenir les chi à l'aide de lignes de commande easieR. Cela se réalise à l'aide de la fonction <code>chi</code>
Cette fonction a 14 arguments mais seuls ceux en **gras** sont importants ici.  

- **X** : character. vecteur des nom des variables dont les modalités sont en en-têtes de lignes. 
- **Y** : character. vecteur des nom des variables dont les modalités sont en en-têtes de colonnes
- **Effectifs** : character. Nom de la variable dans laquelle se trouve les effects. 
- **p** : vecteur de probabilités pour les $\chi^2$ d'ajustement qui servira de distribution théorique. La somme doit valoir 1.  
- **choix** : Type d'analyse à devoir réaliser parmi "Independance", "Ajustement", ou "Test de McNemar" (un seul choix)
- **data** : nom du jeu de données 
- info : logique. Faut-il afficher les aide
- n.boot : nombre de bootstrap pour les analyses plus robustes 
- priorConcentration : paramètre de concentration a priori, fixé à 1 par défaut. (pour plus de détails voir ?contingencyTableBF)
- SampleType : le plan d'échantillonnage. A nouveau, pour plus de détails, voir ?contingencyTableBF
- fixedMargin : pour les plans multinomiaux, quelle est la marge qui est fixe ? (les lignes 'rows' ou les colonnes 'cols')
- choix2 : choix du type de résultats à présenter. Il faut sélectionner un ou plusieurs choix parmi 'Test non parametrique','Test robustes - impliquant des         bootstraps','Facteurs bayesiens'
- rscale : taille d'effet a priori pour le calcul du facteur bayesien 
- **html** : logique. Faut-il afficher la sortie de résultats dans une page html (TRUE) ou seulement dans la console R (FALSE). 


De manière concrète pour réaliser la même analyse que celle réalisée avec les boîte de dialogue, on utilise la fonction suivante : 

```{r eval=F}
chi(X=c('Preference_acteur'),Y=c('Interpretation'),
    Effectifs=NULL, p=NULL, choix='Independance',
    data=Clooney,info=TRUE,n.boot=NULL,priorConcentration =1,
    SampleType=NULL,fixedMargin=NULL,
    choix2=c('Test non parametrique'),
    rscale=0.707, html=T)

```














## 4.2. Interprétez les résultats

La première étape consiste à regarder les effectifs observés. Est-ce que les différentes modalités correspondent effectivement aux modalités qu'on est censé avoir dans le jeu de données. Est-ce que l'effectif total correspond au nombre d'observations que nous sommes censés avoir (Tableau 4). Cette information est également disponible dans la figure, qu'on appelle un spine plot. 

```{r echo=F, message=F, warning=F}
library(huxtable)
chi.out<-chi(X=c('Preference_acteur'),Y=c('Interpretation'),
    Effectifs=NULL, p=NULL, choix='Independance',
    data=Clooney,info=TRUE,n.boot=NULL,priorConcentration =1,
    SampleType=NULL,fixedMargin=NULL,
    choix2=c('Test non parametrique'),
    rscale=0.707, html=F)

tableau<-as.data.frame(chi.out[[1]][[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')

     }}
caption(ht) <-  "Tableau 4. Tableau des effectifs observés."
ht

```


Ensuite, on regarde les effects théoriques. Ce tableau est important car il permet de vérifier les conditions d'applications des $\chi^2$. En effet, les effectifs théoriques (ou attendus) ne doivent pas être inférieurs à 1, et ne doivent pas être inférieurs à 5 dans plus de 20% des situations. Lorsque cette condition n'est pas respectée, il faut s'orienter vers un test alternatif, le Fisher Exact Test. 

```{r echo=F, message=F, warning=F}


tableau<-as.data.frame(chi.out[[1]][[2]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')

     }}
caption(ht) <-  "Tableau 5. Tableau des effectifs attendus"
ht

```


Les tableaux 5 et 6 fournissent la valeur du $\chi^2$, les degrés de liberté et la probabilité associée. Dans le tableau 5 en calculant le $\chi^2$ classique et dans le tableau 6, en calculant le $\chi^2$ de vraisemblance. En l'occurrence, la probabilité est de .1286 pour le classique et de .1204 pour celui de vraisemblance. Cette probabilité étant supérieure à 0.05, on ne peut pas rejeter l'hypothèse nulle. 

Remarquez dans le tableau 5 qu'il est indiquer qu'il n'y a pas la correction de Yates. Cette correction consiste à soustraire 0.5 de chacune des différences entre les effectifs observés et théoriques afin de corriger les petits effectifs. Cette correction ne s'applique que pour les tableaux $2\times2$. Ainsi, pour un tableau $2\times2$, vous aurez une ligne supplémentaire avec cette correction de Yates, qui doit être considérée comme plus robuste. 

On peut remarquer quelques différences par ailleurs entre les deux tableaux. Premièrement, la taille d'effet présentée n'est pas la même. D'un côté, il s'agit du V de Cramer, qu'on peut interpréter comme une sorte de corrélation entre des variables qualitatives. Bien que cela n'ait pas réellement de sens, élevée au carré, on peut l'interpréter comme le pourcentage de variance expliquée. A l'inverse, pour le $\chi^2$ du maximum de vraisemblance, la taille d'effet va être calculée sur la base de la formule fournie par Johston et al. (2006). 

Le second point qui mérite de l'attention est que le tableau 5 fourni la probabilité exacte du test avec le Fisher Exact Test. Ce test représente une alternative plus robuste quand les conditons d'applications ne sont pas respectées. 


```{r echo=F, message=F, warning=F}


tableau<-as.data.frame(chi.out[[1]][[3]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red' )
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')

     }}
caption(ht) <-  "Tableau 6. Tableau du chi²"
ht

```

```{r echo=F, message=F, warning=F}


tableau<-as.data.frame(chi.out[[1]][[4]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red' )

     }}
caption(ht) <-  "Tableau 7. Tableau du chi² du maximum de vraisemblance"
ht

```

Une autre manière de présenter les tailles d'effet sur les variables qualitatives est de présenter un rapport de cote (Odd Ratio en anglais). Le tableau 8 fournit les valeurs des OR. Les valeurs supérieurs à 1 signifie qu'on a X fois plus de chance qu'un événement survienne alors que les valeurs inférieures à 1 signifie qu'on a 1/X fois moins de chances que cet événement survienne.  

Concrètement, la valeur de référence en l'occurrence est "se laisse manipuler". Les fans de Jean Dujardin ont donc 1.778 (1/0.5625) fois moins de chance de dire que Clooney est un gentleman que les fans de Clooney. En revanche, ils ont deux 2.25 fois plus de chance de dire qu'il faut être prêt à tout pour arriver à ses fins. 


```{r echo=F, message=F, warning=F}


tableau<-as.data.frame(chi.out[[1]][[5]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')

     }}
caption(ht) <-  "Tableau 8. Tableau des Odd Ratio"
ht

```

Les tableaux 9 à 11 représente les pourcentages des effectifs qui se retrouvent dans chacune des cases, selon que ce pourcentage est calculé sur le total (Tableau 10), par colonne (Tableau 11) ou par ligne (Tableau 12)


```{r echo=F, message=F, warning=F}


tableau<-as.data.frame(chi.out[[1]][[6]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')

     }}
caption(ht) <-  "Tableau 9. Tableau des pourcentages des effects dans chaque case (calculé sur le total)"
ht

```





```{r echo=F, message=F, warning=F}


tableau<-as.data.frame(chi.out[[1]][[7]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')

     }}
caption(ht) <-  "Tableau 10. Tableau des pourcentages des effects dans chaque case (calculé par colonne)"
ht

```

```{r echo=F, message=F, warning=F}


tableau<-as.data.frame(chi.out[[1]][[8]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')

     }}
caption(ht) <-  "Tableau 11. Tableau des pourcentages des effects dans chaque case (calculé par ligne)"
ht

```


**Note** Si l'effet avait été significatif, une analyse du résidu aurait été ajouté à la sortie de résultats permettant d'identifier quelles cellules contribuent significativement au $\chi^2$. Cette analyse est utile pour des tableaux plus grands que des tableaux $2\times2$. 

## 4.3. Exercices{.tabset .tabset-fade .tabset-pills}   


### Enoncés 

1) Pour l'énoncé suivant, réalisez un test de McNemar en utilisant les boîtes de dialogue.   

Des chercheurs se sont intéressés à l’impact de d’une maladie psychiatrique sur la vie socioprofessionnelle 6 mois après l'annonce de la maladie. 
Ils se sont demandé si l’annonce d’une maladie psychiatrique a des conséquences néfastes sur la situation professionnelle de ces personnes.Ils ont donc mesuré chez des patients le statut professionnel (avec ou sans emploi) au moment de l'annonce et à 6 mois. 

Les données sont issues de la feuille "impact.pro".

Identifiez les similitudes et les différences dans la sortie de résultats. 
Identifiez les similitudes et les différentes dans la ligne de commande utilisée pour réaliser l'analyse de Clonney et cette analyse. 

2) Pour l'analyse suivante, réalisez un $\chi^2$ d'indépendance en utilisant les lignes de commande. 

Relfihcs et Anaurac (2012) se sont intéressés à l’efficacité de deux techniques d’engagement comportemental : « le pied dans la porte » (PP) et « la porte au nez » (PN). Le PP consiste à demander d’abord un comportement peu coûteux à des participants afin d’obtenir le comportement souhaité. Par exemple, on leur demande d’abord de signer une pétition pour la sécurité routière (comportement peu coûteux) avant de leur demander d’installer une pancarte « roulez moins vite » sur la façade de leur maison de façon à ce qu’elle soit visible par les conducteurs (comportement souhaité). La PN relève d’une logique contraire. Elle consiste  à d’abord demander aux participants un comportement trop coûteux (dont on s’attend à ce qu’il soit refusé) afin de les préparer à accepter le comportement souhaité. Par exemple, on leur demande d’abord s’ils sont d’accord d’aider bénévolement tous les week-ends pendant six mois à l’installation de panneaux « roulez moins vite » sur les bords de routes (comportement très coûteux), avant de leur demander d’installer la pancarte sur la façade de leur maison (comportement souhaité). Le but de leur étude était de comparer l’efficacité de ces deux techniques. Les chercheurs ont appliqué le PP sur 50 participants et la PN sur 50 autres participants. Le comportement souhaité était d’obtenir des participants qu’ils s’engagent à économiser l’énergie électrique en coupant certains appareils de leur foyer à certaines heures. Avec le PP, le comportement préparatoire demandé était de signer une pétition pour l’économie d’énergie (peu coûteux). Avec la PN, le comportement préparatoire demandé était de faire du porte-à-porte tous les week-ends, bénévolement, pendant six mois, afin de convaincre les gens d’économiser l’électricité. Les chercheurs ont relevé le nombre de participants ayant accepté/refusé le comportement souhaité. Ils font l’hypothèse que la technique du PP est plus efficace que la technique de la PN.


Les données sont issues de la feuille "engagement".



### Solutions 


1) 
Concernant la sortie de résultats, on identifie qu'il n'y a plus que les effectifs observés. Les effectifs théoriques représente la moitié de la somme de la diagonale des situations incongruentes. Concrètement, ici, il s'agit de 29 et 8. Les effectifs théoriques sont donc : (8+29)/2 = 18.5. 

Dans la ligne de commande, ce qui a changé est évidemement le nom des variables (X et Y), le nom du jeu de données. Par ailleurs, l'argument "Effectifs" est à présent spécifié avec la variable 'Fréquence' et "Independance' a été remplacé par 'Test de McNemar' pour l'argument choix. 

```{r echo=T, message=F, warning=F, eval=F}
chi(X=c( 'A_l_annonce'),
	Y=c('Six_mois_apres_annonce'),
	Effectifs="Frequence", p=NULL, 
	choix='Test de McNemar',data=impact.pro,
	info=TRUE,n.boot=NULL,priorConcentration =1,
	SampleType=NULL,fixedMargin=NULL,
	choix2=c('Test non parametrique'),rscale=0.707, html=F)
```



2)
```{r echo=T, message=F, warning=F, eval=F}
chi(X=c('Techn'),Y=c('Cptmt'),Effectifs=NULL, 
	p=NULL, choix='Independance',data=engagement,info=TRUE,
	n.boot=NULL,priorConcentration =1,SampleType=NULL,fixedMargin=NULL,
	choix2=c('Test non parametrique'),rscale=0.707, html=F) # permet de réaliser un chi carré d'indépendance en ligne de commande
```

Identifiez la présente des analyses sur les résidus dans la sortie de résultats puisque le lien entre la méthode utilisée et l'engagement est significatif. 


# 5. Les corrélations

Afin de pouvoir adopter les bonnes mesures éducatives pour leurs enfants, des chercheurs se sont demandés où ils risquaient le moins d'être arrêtés en possession de marijuana. Ces chercheurs se sont donc demandés s'il existait un lien entre le nombre de colonies d'abeilles productrices de miel dans une région et le nombre d'arrestations pour possession de marijuana. (Cet exemple s'inspire des données présentées ici : http://tylervigen.com/view_correlation?id=1582). 


## Exercices{.tabset .tabset-fade .tabset-pills}   


### Enoncés 

Importer le jeu de données dans la feuille de calcul "miel" en ligne de commande. Donnez lui le nom "miel".  

### Solution 

```{r echo=T, message=F, warning=F}
import(file='Exercices.Bruxelles.xlsx',
       dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
       type='Fichier Excel',
       dec='.',
       sep=';',
       na.strings='NA',
       sheet='miel',
       name='miel')
```

## 5.1. Réaliser une corrélation

### 5.1.1. Corrélations complètes{.tabset .tabset-fade .tabset-pills}   

#### Avec les boîtes de dialogue 



Pour réaliser une corrélation avec <code>easieR</code>, il faut choisir 'analyses - Tests d'hypothèses' (Figure 22). 

<center>![](./corr/corr1.png)</center>	

<small>Figure 22. Dans le premier menu, il faut choisir 'analyses - Tests d'hypothèses'</small>

Ensuite, on choisit corrélations (Figure 23).  

<center>![](./corr/corr2.png)</center>	

<small>Figure 23. Dans le premier menu, il faut choisir 'corrélations'</small>




Le menu suivant permet de choisir le type d'analyses de corrélations qu'on veut réaliser. La différence majeure entre l'analyse détaillée et la matrice de corrélations est qu'il y a plus d'informations fournies avec l'analyse détaillée. Elle est adaptée quand on fait une analyse ou un petit nombre d'analyses. Quand le nombre d'analyses augmente, il est préférable de choisir l'option 'Matrice de corrélations' (Figure 24). Les deux autres options seront développées ultérieurement. 

<center>![](./corr/corr3.png)</center>	

<small>Figure 24. Choisir l'analyse détaillée.'</small>


Pour le moment, nous n'avons pas abordé la distinction entre une corrélation et une corrélation partielle/semi-partielle (Figure 25). Les corrélations renvoient au lien entre deux variables alors que les corrélations partielles renvoient au lien entre des variables quand on a contrôlé l'impact de variables parasites. 

<center>![](./corr/corr4.png)</center>	

<small>Figure 25. Choisir 'Correlations'</small>

On peut directement accéder à cette étape en utilisant la fonction <code>corr.complet</code>

```{r eval=F}
corr.complet()

```


L'étape suivante consiste à choisir le jeu de données. Dans l'exemple présenté, il s'agit de "miel" (Figure 24). Il faut noter que cette étape peut ne pas exister s'il n'y a qu'un seul jeu de données dans la mémoire de R. S'il n'y a pas de jeu de données, <code>easieR</code> va automatiquement proposer d'importer les données. 

<center>![](./corr/corr5.png)</center>	

<small>Figure 25. Choix du jeu de données </small>

On entre à présent dans les choix propres à l'analyse. Si une corrélation est un lien sans direction, la représentation graphique est un nuage de points et il faudra placer une variable en ordonnées et une en abcisse (Figure 26). Dans notre exemple, nous pensons que c'est le nombre de colonies d'abeilles productries de miel (Figure 26, gauche) qui est le prédicteur du nombre d'arrestations pour possession de marijuana (Figure 26, droite). 


<center> ![](./corr/corr6.png)  ![](./corr/corr7.png) </center>	
<small>Figure 26. Choix de la variable en abcisse (à gauche) et en ordonnée (à droite) </small>

Ensuite, on a la possibilité de choisir des sous-groupes. Si on ne veut pas de sous-groupes, cela signifie que l'analyse sera réalisée sur l'intégralité des données. En revanche, si on choisit des sous-groupes, la corrélation sera calculée pour chaque sous-groupe. Cela implique d'avoir une variable qualitative pour laquelle il est possible de subidiviser le jeu de données en un ensemble plus petit de jeux de données. En l'occurence, nous voulons réaliser l'analyse sur l'intégralité des données (Figure 27). 


<center>![](corr/corr8.png)</center>	
<small>Figure 27. En choisissant "non", on fait l'analyse sur l'échantillon complet </small>

L'option suivante permet de décider quelles analyses seront réaliser. L'analyse paramétrique permet d'obtenir la corrélation de Bravais-Pearson, le test non paramétrique permet d'obtenir le $\rho$ de Spearman et le $\tau$ de Kendall. Les statistiques robustes consistent à estimer l'intervalle de confiance autour de la corrélation estimée à l'aide de bootsrap. Enfin, les facteurs bayesiens permettent de réaliser l'analyse en utilisant une approche bayesienne plutôt que fréquentiste (Figure28). 


<center>![](corr/corr9.png)</center>	

<small>Figure 28. Choix des analyses à devoir réaliser. </small>

Comme les statistiques robustes et/ou les facteurs bayesiens ont été choisis, il faut préciser le nombre d'itérations souhaitées. Il faut choisir un grand nombre, avec un minimum de 500. Il est assez fréquent d'utiliser 1000 comme seuil. Cette valeur représente un bon compromis entre précision et vitesse (Figure 29). 

<center>![](./corr/corr10.png)</center>	

<small>Figure 29. Choix du nombre d'itérations pour le bootstrap. </small>

Par ailleurs, comme nous avons opté pour les facteurs bayesiens, il faut fixer un prior. Ce prior est la taille de la corrélation (en valeur absolue) qu'on s'attend à avoir. Si easieR permet de préciser la valeur numérique en ligne de commande, les boîtes de dialogue ne permettent d'utiliser que les étiquettes préprogrammées. En l'occurrence, on a peu de raison de penser que la corrélation va être élevée. Nous optons donc sur la plus petite des taille d'effet (Figure 30). 

<center>![](corr/corr11.png)</center>	

<small>Figure 30. Prior pour les facteurs bayesiens. </small>

La plupart du temps, il est préférable d'analyser les données sans les valeurs influentes. Néanmoins, il y a des exceptions : quand on travaille avec une population pathologique, il est normal de s'attendre à des valeurs influentes. Par ailleurs, certains chercheurs préférent laisser toutes les observations ou utiliser une autre alternative (e.g., les tests non paramétriques) que de supprimer les valeurs influentes. Une attitude minimale est d'aller identifier s'il y a des valeurs influentes. Par défaut, c'est le test de Grubbs qui est utilisé pour identifier la présence des valeurs influentes. Entre l'analyse "sans les valeurs influentes" et "avec les valeurs influentes", il est préférable de choisir "sans" dans la majorité des cas. Néanmoins, je trouve toujours intéressant de regarder si l'interprétation change lorsque les valeurs influentes sont présentes ou non, peu importe le sens. En effet, cela donne une appréciation de la robustesse de l'effet, et permet de s'assurer que la présence/absence de l'effet ne dépend pas que de quelques observations, c'est pourquoi je laisse tout cocher dans la Figure 31. 


<center>![](./corr/corr12.png)</center>	


<small>Figure 31. Attitude par rapport aux valeurs influentes. </small>


La Figure 32 permet d'enregistrer les résultats en format Word. Comme précédemment, nous utiliserons plus loin une autre solution. Nous pouvons cocher FALSE en attendant.  

<center>![](./corr/corr13.png)</center>	

<small>Figure 32. Faut-il enregistrer le document en ms word ? </small>



#### Avec les lignes de commande 

La fonction 'easieR' qui permet de réaliser une analyse de corrélation détaillée est la fonction <code>corr.complet</code>. Les arguments de la fonction sont : 

- X : caractère qui correspond au nom de la variable ou des variables en abcisse ; 

- Y : caractère qui correspond au nom de la variable ou des variables en ordonnée ; 

- Z : caractère qui correspond au nom de la variable ou des variables à contrôler (pour des corrélations parielles) ; 

- data : nom du jeu de données ; 

- group : caractère correspondant à la ou les variables permettant de réaliser les analyses par groupe (voir section consacrée à cet effet) ; 

- param : caractère indiquant quelles analyses doivent être faites. Les valeurs autorisées sont : 'Test parametrique','Test non parametrique','Test robustes - impliquant des bootstraps','Facteurs bayesiens', ou leur version abrégée 'param', 'non param', 'robustes', 'Bayes'

- save : logique indiquant s'il faut une  sauvegarde en fichier msword

- outlier : caractère indiquant si les analyses doivent être réalisées sur l'ensemble des données, s'il faut identifier les valeurs influentes et s'il faut réaliser l'analyse sans les valeurs influentes. Les valeurs autorisées sont : 'Donnees completes','Identification des valeurs influentes','Donnees sans valeur influente'

- z : notez qu'il s'agit d'un z miniscule par opposition du z majuscule présenté précédemment. Pour supprimer les valeurs influente, on peut utiliser le test de Grubbs (par défaut) ou le score z. Si z vaut NULL, le test de Grubbs sera utilisé. Si le z a une valeur numérique, cette valeur numérique sera utilisée pour déterminer l'éloignement en écart-type à partir duquel le résidu standardisé d'une observation va être considéré comme influent. Les valeurs habituelles sont : 1.96 (5%), 2.56 (1%), 3.26 (0.1%). 

- info : il s'agit d'un logique qui indique si des messages d'informations doivent être affichés dans la console pour expliquer à quoi correspond les boîtes de dialogue. Par défaut, la valeur est TRUE. 

- rscale : il s'agit du prior si 'Bayes' ou 'Facteurs bayesiens'est sélectionné pour 'param' . Cela correspond approximativement à la valeur de la corrélation qu'on s'attend à avoir.

- n.boot : nombre du bootstrap pour les statistiques robustes et pour le posterior des facteurs bayesiens 

- html : logique. Indique s'il faut une sortie html. 




```{r, warning=F, message=F}

r.out<-corr.complet(
  X=c('Miel'), 
  Y=c('Marijuana'), 
  Z =NULL,
  data=miel, 
  group=NULL, 
  param=c('Test parametrique','Test non parametrique','Test robustes - impliquant des bootstraps','Facteurs bayesiens'), 
  save=FALSE,
  outlier=c('Donnees completes','Identification des valeurs influentes','Donnees sans valeur influente'),
  z=NULL, 
  info=T, 
  rscale=0.353553390593274, 
  n.boot=1000, 
  html=F)

```


Afin d'illustrer quelques fonctionnalités qui rendent 'easieR' intéressant, voici un exemple où nous avons plusieurs variables en X, plusieurs variables en Y et où on utilise le score z plutôt que le test de Grubbs. Ainsi, on va fixer le seuil du résidu standardisé à 2.56, pour un seuil à 1%. Le jeu de données utilisé est inclu directement dans R et s'appelle 'mtcars'

```{r  message=F, warning=F, include=T, eval=F}
mtcars<-mtcars
r.out<-corr.complet(
  X=c('mpg', "cyl"), 
  Y=c('disp', "hp"), 
  Z =NULL,
  data=mtcars, 
  group=NULL, 
  param=c('Test parametrique'), 
  save=FALSE,
  outlier=c('Identification des valeurs influentes','Donnees sans valeur influente'),
  z=2.56, 
  info=T, 
  rscale=0.353553390593274, 
  n.boot=1000, 
  html=FALSE)
r.out

```


### 5.1.2. Interprétez les résultats


```{r echo=F, message=F, warning=F, include=F}
library(easieR)

r.out<-corr.complet(
  X=c('Miel'), 
  Y=c('Marijuana'), 
  Z =NULL,
  data=miel, 
  group=NULL, 
  param=c('Test parametrique','Test non parametrique','Test robustes - impliquant des bootstraps','Facteurs bayesiens'), 
  save=FALSE,
  outlier=c('Donnees completes','Identification des valeurs influentes','Donnees sans valeur influente'),
  z=NULL, 
  info=T, 
  rscale=0.353553390593274, 
  n.boot=1000, 
  html=FALSE)

```

Pour la sortie de résultats, nous commencerons les explications sur la partie identification des valeurs influentes. La partie sans et la partie avec s'interprétant exactement de la même manière. 


Dans le Tableau 12, nous avons les résultats du test de Grubbs dont l'hypothèse nulle est qu'il n'y a pas de valeurs influentes. 

```{r echo=F, message=F, warning=F}
library(huxtable)
round.ps<-function (x) { substr(as.character(ifelse(x < 0.0001, ' <.0001', 
                                                                ifelse(round(x, 2) == 1, ' >.99', 
                                                                formatC(x, digits = 4, format = 'f')))), 2, 7)}
myf<-function(x){which(x<0.05)}
tableau<-as.data.frame(r.out[[1]][[2]][[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 12. Test de Grubbs testant l'absence de valeurs influentes."
ht

```

Comme la probabilité pour le test de Grubbs est <.001, on rejette l'hypothèse nulle. Cela amène donc à considérer qu'il existe au moins une valeur influente. La valeur influente naturelle sera celle qui s'éloigne le plus de la moyenne :

```{r echo=F}
 r.out[[1]][[2]][[2]]
```

L'important est d'identifier quelles sont les observations influentes et pourquoi elles sont considérées comme influentes. Le Tableau 13 nous permet de le faire. 

```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(r.out[[1]][[2]][[3]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 13. Valeur identifiées comme influentes."
ht

```

En l'occurrence, il apparaît que 1200 colonies d'abeilles est particulièrement faible. En effet, le nombre de colonies d'abeilles est compris entre 2500 et 3500. Il s'agit sans doute d'une erreur. Pour la seconde observations, les choses sont moins claires car, bien que se comportant de manière différente de l'ensemble des autres observations, cette observation, en termes de valeurs, est compatible avec les autres données. Dans notre exemple, cela représente 2 observations sur 200, ce qui représentent moins de 1% des observations considérées comme influentes (Tableau 14). Ce pourcentage étant particulièrement faible, on ne va pas perdre en puissance statistique. On peut donc les supprimer puisqu'il est difficile d'expliquer ces deux observations par rapport aux autres. 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(r.out[[1]][[2]][[4]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 14. Nombre d'observations considérées comme influentes et pourcentage de l'échantillon total."
ht

```

L'étape suivante consiste à regarder les statistiques descriptives. A cette étape, on va être attentif au nombre d'observations (est-ce que je fais bien l'analyse sur tout mon échantiillon), au minimum, au maximum (est-ce que ces valeurs sont compatibles avec mes données), à la moyenne et à la médiane (si elles sont proches, c'est que la distribution est symétrique, sinon ce n'est pas le cas) et éventuellement identifier des tendances (mais ce n'est pas quelque chose qu'on peut faire ici)(Tableau 15). 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(r.out[[1]][[3]][[1]][[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 15. Tableau des statistiques descriptives."
ht

```


L'étape suivante est de regarder les graphiques pour chaque variable (Figure 33). Ces graphiques permettent d'identifier la présence de valeurs influentes (les points qui s'éloignent particulièrement). Dans la Figure de droite, nous n'avons pas encore enlever les valeurs influentes, dans les figures de gauche les valeurs influentes ont été enlevées. Les graphiques du haut représent les colonies d'abeilles et ceux du bas représentent les arrestations. Il ne s'agit pas de la disposition habituelle dans la sortie easieR, mais cette comparaison permet d'identifier la modification de la distribution et comment on peut identifier des valeurs influentes sur la base d'un graphique. 

```{r, echo=F, warning=F, message=F}
p1<-r.out[[1]][[1]][[1]][[2]][[1]]
p2<-r.out[[1]][[3]][[1]][[2]][[2]]
p3<-r.out[[1]][[3]][[1]][[2]][[1]]
p4<-r.out[[1]][[3]][[1]][[2]][[2]]
.multiplot(p1,p2,p3,p4, cols=2)

```

<small>Figure 33. Représentation graphique des différentes variables</small>


Pour identifier la forme de la distribution, il suffit de faire un quart de tour avec sa tête d'un côté ou de l'autre. Si vous tracez une ligne verticale entre au milieu de la distribution, cela donne la forme de votre distribution. La Figure 34 illustre ce qu'on devrait obtenir pour une distribution normale. 

```{r}
y<-rnorm(10000)
data<-data.frame(y) 
p<-ggplot(data=data, aes(x=factor(0) ,y=y))+geom_violin()
p<-p+geom_vline(xintercept=factor(0), size=1.5)
p

```


<small>Figure 34. Graphique violon pour une distribution normale.</small>

```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(r.out[[1]][[3]][[2]][[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 17. Tests de normalité."
ht

```

Deux tests de normalité ont été réalisé. Le premier (et le plus puissant) est le test de Shapiro-Wilk. La probabilité associée à ce test est de 0.13. Elle est supérieure à 0.05,  on tolère l'hypothèse nulle selon laquelle la distribution des données ne se distingue pas d'une distribution normale. Le test de Lilliefors fonctionne sur le même principe : avec une probabilité de 0.35, on tolère l'hypothèse nulle selon laquelle la distribution suit une distribution normale. Ainsi, ici les deux tests aboutissent à la même conclusion. Lorsque les deux tests n'aboutissent pas à la même conclusion, il faut privilégier le test de Shapiro-Wilk lorsque l'échantillon est petit (moins de 50 personnes) car ce test est plus puissant et le test de Lilliefors pour les grands échantillons (plusieurs centaines de personnes). En effet, avec les gros échantillons, de minimes violations de la normalité aboutissent au rejet de l'hypothèse nulle avec le test de Shapiro-Wilk. 


```{r echo=F, message=F, warning=F}

p1<-r.out[[1]][[3]][[2]][[2]]
p2<-r.out[[1]][[3]][[2]][[3]]
.multiplot(p1, p2, cols=2)

```

<small>Figure 34. Distribution des résidus en fonction d'une distribution normale théorique et QQplot </small>

Si, sur la distribution des résidus, on peut identifier des divergences entre la distribution des résidus et la distribution normale, le QQplot suit plutôt bien une ligne droite ce qui permet de considérer que la distribution est normale. 

Nous pouvons donc à présent regarder si la distribution des résidus est homogènes : 

```{r echo=F, message=F, warning=F}

p<-ggplot(data=miel, aes( x=Miel, y=Marijuana))+geom_point()
p<-p+geom_smooth(method="lm")
p<-p+geom_segment(x=2500, y=85000, xend=2500, yend=95000, colour ="red", size=1)
p<-p+geom_segment(x=2955, y=38000, xend=2955, yend=65000, colour ="green", size=1)
p
```

<small>Figure 35. Nuage de points pour la corrélation de Bravais-Pearson. On constate que la ligne verticale rouge est bien plus courte que la ligne verticale verte. </small>

Sur ce nuage, on peut constater que les variances ne sont pas homogènes. En effet, a ligne verticale rouge est bien plus courte que la ligne verticale verte. Ainsi, on devrait privilégier les statistiques robustes ou le $\rho$ de Spearman. 

L'étape suivante consiste à analyser les résultats de la corrélation (Tableau 18)


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(r.out[[1]][[3]][[3]][[2]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 18. Corrélation de Bravais-Pearson."
ht

```

La corrélation de Bravais-Pearson vaut -0.91. La taille d'effet R² vaut 0.83. Le calcul de la statistique *t* aboutit à une valeur de -31, et avec 198 degrés de liberté, la probabilité associée est <.001. Ainsi, on rejette l'hypothèse nulle. La corrélation est significative (i.e., elle ne vaut pas 0) et elle est négative. L'intervalle de confiance créé par boostrap (donc les statistiques robustes) indiquent que la vraie corrélation à 95% de chance de se situer entre -0.932 et -0.882. Comme cet intervalle de confiance ne recouvre pas le 0, il est très probable que la valeur de la vraie corrélation ne soit pas égale à 0. 

Etant donné que nous avons coché la case relative aux tests non paramétriques, le $\rho$ de Spearman est également réalisé. Ainsi, comme cette analyse est une corrélation de Bravais-Pearson sur les rangs, la représentation graphique des données consiste à représenter les rangs des observations (Figure 36). 


```{r echo=F, message=F, warning=F}

r.out[[1]][[3]][[4]][[1]]
```

<small>Figure 36. Nuage de points pour le $\rho$ de Spearman. </small>

Le tableau du $\rho$ de Spearman (Tableau 19) s'interprète de la même manière que celui de la corrélation de Bravais-Pearson, excepté que la valeur de la statistique est un *S* au lieu d'un *t*. 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(r.out[[1]][[3]][[4]][[2]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 19. Rho de Spearman."
ht

```

Le $\tau$ de Kendall est une autre alternative non paramétrique à la corrélation de Bravai-Pearson. Les puristes pourraient préférer cette mesure car elle est plus précise que le $\rho$ (Tableau 20). 

```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(r.out[[1]][[3]][[4]][[3]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 20. Tau de Kendall."
ht

```

Il est intéressant de constater que, si les trois analyses aboutissent à la conclusion qu'il existe une corrélation négative, la valeur du r de Bravais-Pearson est de -0.91, celle du $\rho$ est de -0.69 et celle du $\tau$ est de -0.48. Cela permet de prendre conscience de la différence en termes d'estimation quand les conditions d'application sont respectées et lorsqu'elles ne le sont pas. Pour l'illustrer, ci-dessous, vous pouvez comparer la valeur de la corrélation pour le r de Bravais-Pearson et le $\rho$ de Spearman lorsque les conditions d'application sont respectées. Vous pouvez observer que ces deux valeurs sont très similaires. 
 
```{r echo=T, message=F, warning=F}

samples = 200
r = 0.83

library('MASS')
data = mvrnorm(n=samples, mu=c(0, 0), Sigma=matrix(c(1, r, r, 1), nrow=2), empirical=TRUE)
X = data[, 1]  
Y = data[, 2]  

corr.test(X, Y, method = "pearson") 
corr.test(X, Y, method = "spearman") 


``` 



### 5.1.3 Exercices{.tabset .tabset-fade .tabset-pills} 

#### Enoncés

**Exercice 1**

Des chercheurs s’intéressent au niveau de détresse émotionnelle et formulent l’hypothèse selon laquelle la détresse serait inversement reliée à l’adoption de stratégies de coping. Ces chercheurs demandent  à leurs participants d’évaluer sur une échelle de 1 (pas du tout) à 7 (en totale détresse) leur niveau de détresse émotionnelle. Ils leur demandent ensuite d’expliquer les comportements qu’ils adoptent lorsqu’ils ne se sentent pas bien. Les chercheurs attendent 30 réponses. Une fois, les réponses récoltées, ils comptent le nombre de comportements de coping. Par ailleurs, les  chercheurs ont également utilisé une échelle d’anxiété sur 50 et ont demandé l’âge des participants. 

Réaliser une corrélation partielle entre la détresse émotionnelle et les stratégies de coping, en contrôlant l'effet de l'âge. Comparez la sortie de résultats avec celle d'une corrélation classique. 

Les données sont présentées la feuille 'Detresse'. Utilisez au choix les boites de dialogue ou la ligne de commande. 



**Exercice 2**

Aux USA, il existe une forte une corrélation entre la consommation de margarine et le taux de divorce. Nous émettons l'hypothèse que ce n'est pas la consommation de margarine en tant que telle qui influence le taux de divorce, mais que le taux de divorce augmente quand la consommation de maragrine augmente en raison de la prise poids. Testez cette hypothèse. 

<center>![](poids.gif)</center>

En vous appuyant sur la ligne de commande issu de l'exercice 1, réaliser une corrélation partielle en utilisant la ligne de commande. 

Les données sont dans la feuille de calcul 'poids'. 


**Exercice 3**

Pour les plus rapides, importer la feuille de calcul AI, réaliser une corrélation entre le BLOC1 et le BLOC2 en faisant l'analyse par groupe. Faites cette analyse au choix avec les boîtes de dialogue ou les lignes de commande. 


#### Solutions 


**Exercice 1**

```{r echo=T, message=F, warning=F, eval=F}

import(file='Exercices.Bruxelles.xlsx',
       dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
       type='Fichier Excel',
       dec='.',
       sep=';',
       na.strings='NA',
       sheet='Detresse',
       name='Detresse')

corr.complet(
  X=c('detresse'), 
  Y=c('coping'), 
  Z =c('age'),
  data=Detresse, 
  group=NULL, 
  param=c('Test parametrique','Test non parametrique'), 
  save=FALSE,
  outlier=c('Donnees completes','Identification des valeurs influentes','Donnees sans valeur influente'),
  z=NULL, 
  info=T, 
  rscale=0.353553390593274, 
  n.boot=1000, 
  html=T)

``` 


La sortie de résultats sont sensiblement les mêmes, mais avec une différence sur le tableau de corrélation : on obtient la valeur de la corrélation partielle et semi-partielle. 

**Exercice 2**
```{r echo=T, message=F, warning=F, eval=F}

import(file='Exercices.Bruxelles.xlsx',
       dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
       type='Fichier Excel',
       dec='.',
       sep=';',
       na.strings='NA',
       sheet='poids',
       name='poids')

corr.complet(
  X=c('margarine'), 
  Y=c('divorces'), 
  Z =c('prise_poids_10ans'),
  data=poids, 
  group=NULL, 
  param=c('Test parametrique','Test non parametrique'), 
  save=FALSE,
  outlier=c('Donnees completes','Identification des valeurs influentes','Donnees sans valeur influente'),
  z=NULL, 
  info=T, 
  rscale=0.353553390593274, 
  n.boot=1000, 
  html=T)

``` 


**Exercice 3**

```{r echo=T, message=F, warning=F, eval=F}

import(file='Exercices.Bruxelles.xlsx',
       dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
       type='Fichier Excel',
       dec='.',
       sep=';',
       na.strings='NA',
       sheet='AI',
       name='AI')

corr.complet(
  X=c('BLOC1'), 
  Y=c('BLOC2'), 
  Z =NULL,
  data=AI, 
  group="TYPE", 
  param=c('Test parametrique','Test non parametrique'), 
  save=FALSE,
  outlier=c('Donnees completes','Identification des valeurs influentes','Donnees sans valeur influente'),
  z=NULL, 
  info=T, 
  rscale=0.353553390593274, 
  n.boot=1000, 
  html=T)

``` 


## 5.2. Les matrices de corrélations

Pour traiter de la problématique des matrice de corrélations, nous allons utiliser les données du Big Five Inventory disponible dans le package 'psych'. Comme le package a été chargé grâce à easieR, on peut sans difficulté y accéder de la manière suivante : 

```{r echo=T, message=F, warning=F, eval=T}

bfi<-bfi # permet aux données bfi stockées dans le package psych d'être actives dans la mémoire de R. 

``` 

### 5.2.1. Réaliser une matrice de corrélations {.tabset .tabset-fade .tabset-pills}  

#### Avec les boîtes de dialogue 

Comme précédemment, on peut accéder à la matrice de corrélations en partant de la fonction <code>easieR</code>, mais nous ne reprendrons pas le début de la procédure. 

Nous commencerons à développer la partie de la procédure à laquelle on peut accéder grâve à la fonction <code>corr.matrice()</code>.

Le premier choix consiste à déterminer si on veut faire une matrice de corrélations ou une matrice de  corrélations partielles (Figure 37). 


<center>![](./corr/corr14.png)</center>

<small>Figure 37. Choix du type de corrélations désirées. </small>

Une fois ce choix réalisé, il faut préciser le jeu de données sur lequel les analyses vont être réalisées. S'il n'y a qu'un jeu de données dans la mémoire de R, ce sera ce jeu de données qui sera choisi par défaut. En l'occurrence, ce sera le bfi (Figure 38).


<center>![](./corr/corr15.png)</center>

<small>Figure 38. Choix du jeu de données. </small>

Les matrices de corrélations peuvent être carrées ou rectangulaires. 

Une matrice carrée est une matrice pour laquelle on fait les corrélations de l'ensemble des variables entre elles. 

```{r echo=F, message=F, warning=F, eval=T}
round(corr.test(bfi[,1:5])$r,3)


``` 
A l'inverse, dans une matrice rectangulaire, on fait les corrélations entre un premier jeu de variables avec un second jeu de variables. 

```{r echo=F, message=F, warning=F, eval=T}
round(corr.test(bfi[,1:3], bfi[,4:5])$r,3)


``` 

On va privilégier la matrice rectangulaire par rapport à la carrée quand on va tester des hypothèses et qu'on va interpréter les corrélations significatives. En effet, on ne testera que les corrélations qui correspondent à des hypothèses diminuant donc le nombre de corrélations et la correction de la probabilité appliquée. 

On privilégie les matrices carrées quand l'objectif est plus exploratoire ou quand il s'agit d'une étape préalable à une analyse factorielle par exemple. 

Il va de soi qu'il y a des situations où les matrices carrées pourront être utilisées pour répondre à des hypothèses. 

Dans ce cas, nous allons réaliser une matrice de corrélations carrée (Figure 38). 

<center>![](./corr/corr16.png)</center>

<small>Figure 38. Choix du type de matrice de corrélations désirées. </small>

L'étape suivante consiste à choisir les variables d'intérêt. En l'occurrence, nous choisirons les 5 variables d'agréabilité (Figure 39).

Si nous avions choisi un matrice rectangulaire, il aurait fallu choisir un premier jeu de données (par exemple les variables A1, A2 et A3) et ensuite un second jeu de données (par exemple A4 et A5). 

<center>![](./corr/corr17.png)</center>

<small>Figure 39. Choix des variables pour la matrice de corrélations. </small>

Comme pour l'analyse complète, on peut faire les matrices de corrélations par sous-groupes. Si on choisit 'oui' (Figure 40), il faudra préciser la variable qualitative qui permettra de subdiviser l'échantillon en plusieurs sous-groupes. 

<center>![](./corr/corr8.png)</center>

<small>Figure 40. Faut-il réaliser les analyses par sous-groupe ?. </small>





Comme pour l'analyse complète, la question qui se pose est de savoir si on fait la matrice de corrélations sur les données complètes ou sur les données pour lesquelles on a enlevé les valeurs influentes. Les valeurs influentes sont déterminées sur la base de la distance de Mahalanobis (Figure 39). 

<center>![](./corr/corr18.png)</center>
<small>Figure 39. Faut-il supprimer les valeurs influentes avant de faire les analyses ?. </small>

En l'occurrence, nous avons choisi de ne pas les supprimer. Si nous avions choisi de les supprimer, Lorsque les choix auront été faits, easieR vous aurait averti du nombre d'observations considérées comme influentes (Figure 41). En l'occurrence, 1.11% des observations sont considérées comme influentes. 

<center>![](./corr/corr24.png)</center>

<small>Figure 41 Pourcentage d'observations considérées comme influentes. </small>

Lorsque ce pourcentage est différent de 0%, easieR va vous proposer de les supprimer toutes en une fois ou une par une. Le fait de les supprimer une par une afin de vous permettre de les identifier et comprendre pourquoi elles sont influentes. En l'occurrence, à titre de démonstration, nous les enlèverons une par une (Figure 42). 

<center>![](./corr/corr25.png)</center>

<small>Figure 42. Faut-il supprimer toutes les observations ou analysez chaque observation considérée comme influente manuellement. </small>

Quand on choisit de supprimer les observations manuellement, la valeur la plus extrême avec l'identifiant du participant et sa distance de Mahalanobis est affiché dans la console (Figure 43). Il faut appuyer sur entrée pour continuer. 


<center>![](./corr/corr26.png)</center>

<small>Figure 43. Valeur la plus extrêment considérée comme influente. </small>

Après avoir analysé l'observation, easieR va vous demander si vous souhaitez la supprimer (Figure 44). Si vous cliquez que "oui", easieR va supprimer cette observation, et détecter l'observation suivante la plus influente. Cette opération est réitérée jusqu'à ce plus aucune observation ne soit considérée comme influente ou jusqu'à ce que l'utilisateur ne veuille plus supprimer les observations. 

<center>![](./corr/corr27.png)</center>

<small>Figure 44. Faut-il supprimer la dernière observations affichée dans la console et considérée comme étant la plus influente encore dans le jeu de données. </small>

**ATTENTION** Dans le cas d'une analye individuelle, il peut y avoir un problème de reproductibilité du code. Quand on utilise easieR en ligne de commande, il peut faire l'analyse sur l'échantillon complet ou sur l'échantillon sans les valeurs influentes. En revanche, la ligne de commande ne prévoit pas la situation où seules certaines observations influentes sont supprimées de l'échantillon. Si naturellement, on pourrait considérer qu'on enlève ou non les valeurs influentes, des situations intermédiaires pourraient survenir, à la condition d'avoir fixé le critère de suppression A PRIORI. 


L'étape suivante consiste à choisir le type de corrélations que vous souhaitez. Naturellement, on a tendance à choisir les corrélations de Bravais-Pearson. Notez néanmoins que les variables étant ordinales ici, les $\rho$ de Spearman ou les $\tau$ de Kendall aurait pu/dû être privilégiés (Figure 45).

<center>![](./corr/corr19.png)</center>
<small>Figure 45.Type de matrice de corrélations qu'il faut réaliser. </small>

Vous aurez encore la possibilité de faire l'analyse en adoptant les tests d'hypothèse nulle, les facteurs bayesiens ou les deux. En l'occurrence, nous focaliserons uniquement sur les tests d'hypothèse nulle. 

<center>![](./corr/corr20.png)</center>
<small>Figure 46. Choix entre tests d'hypothèse nulle, facteurs bayesiens ou les deux. </small>

Lorsque les tests d'hypothèse nulle ont été choisis, il faut corriger la probabilité pour éviter la multiplication de l'erreur de première espèce. LA correction la plus connue est la correction de Bonferroni mais la correction de Holm représente un choix plus judificieux. En effet, cette correction contrôle autant le risque de commettre une erreur de première espèce mais contrôle mieux le risque de commettre une erreur de seconde espèce. 

<center>![](./corr/corr21.png)</center>

<small>Figure 47. Choix entre tests d'hypothèse nulle, facteurs bayesiens ou les deux. </small>

Lorsque des valeurs sont manquantes dnas le jeu de données, easieR vous force à prendre une décision. Il commence par indiquer que certaines observations sont manquantes (Figure 48).

<center>![](./corr/corr22.png)</center>

<small>Figure 48. easieR indique la présence de valeurs manquantes sur les variables qui sont analysées. </small>

Il faut donc prendre une décision (Figure 49). Une première décision peut être de faire les analyses avec les informations disponibles et de continuer avec l'échantillon tel qu'il est. Cela peut se justifier sur un grand échantillon où il y a peu de valeurs manquantes. En revanche, c'est plus embêtant quand les valeurs manquantes sont nombreuses sur certaines variables et beaucoup moins fréquentes sur d'autres. La raison du souci est que la significativité n'est plus estimée en utilisant les mêmes degrés de liberté. Plusieurs solutions peuvent être envisagées pour avoir l'ensemble des corrélations calculées sur la même taille d'échantillon. On peut supprimer les observations où il y a des valeurs manquantes (au risque de perdre en puissance statistique). On peut égaleent remplacer les observations par la moyenne (pour les variables continues) ou par la médiane (pour les variables continues et ordinales). Enfin, il est possible de s'appuyer sur les corrélations avec les autres variables pour imputer la valeur que le participant aurait dû avoir au regard de ses scores aux autres variables. C'est ce qu'on appelle l'imputation multiple. Il existe plusieurs méthodes, Dans le cas de easieR, il s'appuie sur le package Amelia. 

<center>![](./corr/corr23.png)</center>

<small>Figure 49. Choix possibles pour traiter les valeurs manquantes. </small>

Comme précédemment, on termine en décidant si on souhaite sauvegarder les résultats. 

#### Avec les lignes de commande 

A nouveau, il est possible de réaliser des matrices de corrélations en utilisant la fonction <code>corr.matrice()</code>


- X : caractère qui correspond au nom du premier ensemble de variables (si Y n'est pas précisé, crée une matrice carrée) ; 

- Y : caractère qui correspond au nom du second ensemble de variables ; 

- Z : caractère qui correspond des variables variables à contrôler (pour des corrélations parielles) ; 

- data : nom du jeu de données ; 

- group : caractère correspondant à la ou aux variables permettant de réaliser les analyses par groupe (voir section consacrée à cet effet) ; 

- param : caractère indiquant quelles analyses doivent être faites. Les valeurs autorisées sont : 'Test de H0', 'Facteurs bayesiens', ou leur version abrégée, à savoir 'FB' et 'H0'. 

- method : type de corrélations souhaitées. Il s'agit d'une valeur de type caractère parmi "pearson", "spearman", "kendall".

- p.adjust : type de correction des probabilités pour contrôler l'erreur de famille. Il s'agit une valeur caractère à choisir parmi : "holm", "hochberg", "hommel", "bonferroni", "BH", "BY","fdr", "none"

- save : logique indiquant s'il faut une  sauvegarde en fichier msword

- outlier : caractère indiquant si les analyses doivent être réalisées sur l'ensemble des données, s'il faut identifier les valeurs influentes et s'il faut réaliser l'analyse sans les valeurs influentes. Les valeurs autorisées sont : 'Donnees completes','Donnees sans valeur influente'.

- info : il s'agit d'un logique qui indique si des messages d'informations doivent être affichés dans la console pour expliquer à quoi correspond les boîtes de dialogue. Par défaut, la valeur est TRUE. 

- rscale : il s'agit du prior pour les facteurs bayesiens. Cela correspond approximativement à la valeur de la corrélation qu'on s'attend à avoir.

- n.boot : nombre du bootstrap pour les statistiques robustes et pour le posterior des facteurs bayesiens 

- html : logique. Indique s'il faut une sortie html. 


```{r echo=T, message=F, warning=F, eval=F}
corr.matrice(X=c('A1','A2','A3','A4','A5'), 
		Y=NULL, Z =NULL,data=bfi, method="pearson",
		p.adjust='holm', group=NULL, 
		param=c('Tests de H0'), save=FALSE,
		outlier=c('Donnees completes'), info=T, rscale=NULL, n.boot=1, html=T)	

``` 

**ATTENTION** Dans le cas où il y a des valeurs manquantes qui n'ont pas été traitées avant de réaliser l'analyse, easieR va le détecter comme une décision à prendre, il faut donc terminer l'analyse avec les boîtes de dialogue. 

### 5.2.2. Interpréter la sortie de résultats

```{r echo=F, message=F, warning=F, eval=T}
cor.out<-corr.matrice(X=c('A1','A2','A3','A4','A5'), 
		Y=NULL, Z =NULL,data=bfi, method="pearson",
		p.adjust='holm', group=NULL, 
		param=c('Tests de H0'), save=FALSE,
		outlier=c('Donnees completes'), info=T, rscale=NULL, n.boot=1, html=T)	

``` 

Dans la sortie de résultats, le premier tableau (Tableau 21). A cette étape, on va être attentif au nombre d'observations (est-ce que je fais bien l'analyse sur tout mon échantiillon), au minimum, au maximum (est-ce que ces valeurs sont compatibles avec mes données), à la moyenne et à la médiane (si elles sont proches, c'est que la distribution est symétrique, sinon ce n'est pas le cas).  Remarquez en l'occurrence, qu'il peut y avoir jusqu'à une centaine d'observations en moins sur certaines variables. Cela doit donc amener à s'interroger sur la pertinence d'avoir choisi de ne rien faire pour les valeurs manquantes. 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(cor.out[[1]][[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 21. Statistiques descriptives pour l'ensemble des variables de l'analyse."
ht

```

Le second tableau permet de vérifier si la multinormalité est respectée (Tableau 22). Cette multinormalité n'est affichée que pour les corrélations de Bravais-Pearson. En l'occurrence, ce n'est pas le cas. En effet, tant la symétrie (skewness) n'est pas respectée (il y a asymétrie) et il y a soit un aplatissement ou plutôt pointue. En l'occurrence, la valeur étant plutôt élevée, on doit considérée que la distribution est trop pointue pour une distribution normale.  Il aurait donc été préférable de faire des $\rho$ de Spearman ou des $\tau$ de Kendall. 




```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(cor.out[[1]][[2]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 22. Tableau testant la multinormalité."
ht

```



Le troisième tableau est le tableau avec les corrélations (Tableau 23). 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(cor.out[[1]][[3]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 23. Matrice de corrélations."
ht

```


Comme le nombre d'observations n'est pas le même pour toutes les variables, le tableau suivant informe sur le nombre d'observations utilisées pour chaque corrélation 2 à 2 (Tableau 24). 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(cor.out[[1]][[4]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 24. Nombre d'observations pour chaque combinaison de variables 2 à 2."
ht

```

Le Tableau 25 donne les probabilités associées à chacune des probabilités. Il faut noter ici que les données au-dessus de la diagonales ont été corrigées en appliquant la correction de Holm (notre choix), tandis que celles en-dessous n'ont pas été corrigées. Cela permet d'identifier les corrélations qui sont significatives sans corrections et qui deviennent non significatives après correction. Ces corrélations sont sans doute de l'erreur de première espèce. En l'occurrence, corrigées ou non, toutes les corrélations sont significatives. 



```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(cor.out[[1]][[6]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 25. Matrice des probabilités."
ht

```


Il est toujours appréciable d'identifier le pourcentage de variance expliquée lorsqu'on réalise une corrélation. La matrice des R² (Tableau 26) fournit cette information. 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(cor.out[[1]][[7]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 26. Pourcentage de variance expliquée pour chaque corrélation."
ht

```

Pour terminer,il y a un tableau de synthèse avec l'intervalle de confiance, la valeur de la corrélation et la probabilité associée. En l'occurrence, l'intervalle de confiance sur la corrélation est calculé sur la base d'une distribution normale. Si nous avions choisi les statistiques robustes, il aurait été estimé sur la base de boostrap (Tableau 27)


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(cor.out[[1]][[8]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 27. Corrélation, intervalle de confiance autour de la corrélation et probabilité associée à la valeur de la corrélation."
ht

```


Par ailleurs, noter dans la sortie HTML, qu'il y a une page qui s'ouvre avec la matrice uniquement, comprenant la valeur de la corrélation, la probabilité et le pourcentage de variance expliquée. L'objectif est de facilité les copier-coller quand on veut reporter la matrice dans un article. 



### 5.2.3. Exercices {.tabset .tabset-fade .tabset-pills}

#### Enoncés


Un professeur de statistiques veut développer un nouvel outil pour mieux cerner les difficultés de ses étudiants. Il se demande s’il va pouvoir utiliser son questionnaire pour pouvoir apporter l’aide appropriée en fonction des difficultés de ses étudiants. Voici le questionnaire :


Pour les items qui suivent, veuillez préciser à quel point vous vous reconnaissez sur une échelle allant de fortement en désaccord (FD = 1), désaccord (D), neutre (N), accord (A), fortement d’accord (FA = 5). 
(cet exemple est inspiré de Field et al., 2013)

1. Les statistiques me font pleurer					
2. Mes amis pensent que je suis stupide de ne pas être capable d’utiliser R					
3. Les écart-types, ça m’excite					
4. Je rêve que Pearson m’attaque avec des coefficients de corrélation					
5. Je ne comprends rien aux statistiques					
6. J’ai peu d’expérience avec les ordinateurs					
7. Les ordinateurs me détestent					
8. Je n’ai jamais été bon en mathématique					
9. Mes amis sont meilleurs que moi en statistiques					
10. Les ordinateurs, c’est seulement fait pour jouer					
11. Je m’en sortais mal en mathématiques à l’école					
12. Les gens essaient de t’expliquer que R rend les statistiques plus simples à comprendre, mais c’est pas vrai					
13. Je m’inquiète de créer des dégâts irréparables sur les ordinateurs en raison de mon incompétence					
14. Les ordinateurs ont leur propre volonté et décident volontairement de ne pas fonctionner quand je les utilise					
15. Les ordinateurs ne me comprennent pas					
16. Je sanglote ouvertement quand j’entends « tendance centrale »					
17. Je m’évanouis dès que je vois une équation					
18. R crash chaque fois que je l’utilise					
19. Tout le monde me regarde quand j’utilise R					
20.. Je n’arrive plus à dormir parce que je rumine sur les valeurs propres					
21. Je me réveille sous ma couverture en étant persuadé que je suis piégé sous une distribution normale					
22. Mes amis sont meilleurs que moi pour utiliser R					
23. Si je suis bon en statistiques, mes amis vont penser que je suis un nerd					


Importer le fichier diff.stat, réaliser une matrice de corrélations carrée sur l'ensemble des variables. Analysez la sortie de résultats. 
Vous pouvez le faire en boîte de dialogue ou en ligne de commande. Identifiez les corrélations qui ne sont pas significatives après correction alors qu'elles le sont avant correction. 

#### Correction 

Pour les personnes qui ont choisi de réaliser l'analyse en ligne de commande, il peut être fastidieux de renommer l'ensemble des variables. On peut aller beaucoup plus vite grâve à la fonction <code>dput</code> qui n'est pas dans easieR. L'idée est de faire un vecteur avec le nom des variables : 

```{r echo=T, message=F, warning=F, eval=T}
import(file='Exercices.Bruxelles.xlsx',
       dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
       type='Fichier Excel',
       dec='.',
       sep=';',
       na.strings='NA',
       sheet='diff.stat',
       name='diff.stat')

dput(names(diff.stat)) 
```


On peut ensuite réaliser la matrice de corrélation en faisant un copier coller du vecteur. 

```{r echo=T, message=F, warning=F, eval=F}
corr.out<-corr.matrice(X=c("Q01", "Q02", "Q03", "Q04", "Q05", "Q06", "Q07", "Q08", "Q09", 
				"Q10", "Q11", "Q12", "Q13", "Q14", "Q15", "Q16", "Q17", "Q18", 
					"Q19", "Q20", "Q21", "Q22", "Q23"), 
		Y=NULL, Z =NULL,data=diff.stat, 
		p.adjust='holm', group=NULL, 
		param=c('Tests de H0'), save=FALSE,
		outlier=c('Donnees completes'), info=T, rscale=NULL, n.boot=1)



```


Il y a de nombreuses corrélations qui perdent leur significativité après correction. C'est notamment le cas entre la variable 2 et 6. 



# 6. Les régressions {.tabset .tabset-fade .tabset-pills}

## Enoncé Femme
Des chercheurs se sont demandés quels sont les facteurs qui contribuaient le plus au sex appeal. Concrètement, ils ont demandé à des jeunes femmes à quel point, le charme, l'humour, l'élégance et l'intelligence représentait des facteurs qui les attiraient. 

Pour les aider à se projeter, les auteurs de l'étude ont décidé de proposer d'estimer à quel point un homme qui avait le charme de Johnny Depp les attirait ; à quel point un homme qui avait l'humour de Jean Dujardin les attirait ; à quel point un homme qui avait l'intelligence de Leonardo Di Caprio les attirait ; et enfin à quel point un homme qui avait l'élégance de Georges Clooney les attirait. 

<center>![](./reg/Diapositive5.png)</center>

Les données sont présentes dans la feuille 'sex_appeal' et sont présentées dans le Tableau 28.

```{r, echo=F}
library(easieR)
library(DT)
import(file='Exercices.Bruxelles.xlsx',
	dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
	type='Fichier Excel',dec='.',sep=';',na.strings='NA',sheet='sex_appeal',name='sex.appeal')
datatable(sex.appeal, caption="Tableau 28. Jeu de données Sex_appeal")

```

Les auteurs ont également réalisé une version destinée aux hommes...


## Enoncé Homme 

Aux hommes, ils ont demandé à quel point une femme qui avait le charme de Nathalie Portman les attirait ; à quel une femme qui avait l'humour ... de Nathalie Portman les attirait ; à quel une femme qui avait l'intelligence ... de Nathalie Portman les attirait ; à quel une femme qui avait l'élégance ... de Nathalie Portman les attirait ;


<center>![](./reg/Diapositive9.png)</center>

#
## 6.1. Réaliser une régression {.tabset .tabset-fade .tabset-pills}

### Avec les boîtes de dialogue 

Pour accéder directement aux régressions, nous utiliserons la fonction <code>regressions()</code>. 

La première boîte de dialogue consiste à décider si on veut un modèle où les relations entre les variables sont linéaires, un modèle où les variables ont des effets de modération entre elles (effets interactifs) et vous pouvez spécifier votre modèle vous-même (Figure 50). Dans notre cas, un modèle additif conviendra parfaitement.  

<center>![](./reg/reg1.png)</center>
<center><small>Figure 50. Choix du type de modèle qu'il faudra réalisé </small></center>

La seconde (Figure 51 gauche) et troisième étape (Figure 51 droite) consiste à choisir respectivement la variable dépendante et les variables indépendantes.  

<center>![](./reg/reg2.png)</center><center>![](./reg/reg3.png)</center>
<center><small>Figure 51. Choix de la variable dépendante (gauche) et des variables indépendantes (droite) </small></center>

Si vous aviez choisi de spécifier votre modèle dans la Figure 50 (voir Figure 52 gauche), vous auriez eu une nouvelle boîte de dialogue vous donnant le modèle établi par les boîtes de dialogue que vous pouviez modifier à votre guise (Figure 52 droite). 

<center>![](./reg/reg10.png)</center><center>![](./reg/reg11.png)</center>
<center><small>Figure 51. Choix d'un modèle qu'on veut spécifier soi-même (gauche) et boîte de dialogue permettant de spécifier ce modèle (droite) </small></center>

Cette manière de faire vous permet de vous familiariser avec la manière d'écrire les modèles de manière classique dans R. Cela peut représenter un bel exercice. 

Dans la boîte de dialogue suivante (Figure 52), vous pourrez choisir le type de tests que vous souhaitez obtenir. Comme précédemment, nous choisirons les tests paramétriques uniquement, les tests robustes et les facteurs bayesiens dépassant le cadre de cette formation. 

<center>![](./reg/reg4.png)</center>
<center><small>Figure 52. Type d'analyse à devoir réaliser. En l'occurrence seules les régressions paramétriques sont réalisées </small></center>

Vous devez ensuite décider si vous souhaitez réaliser les analyses sur les données complètes, identifier les valeurs influentes et si vous souhaitez les analyses sans les valeurs influentes. Ici, la valeur par défaut est de considérer une valeur influente quand sa distance de Cook dépasse le seuil 4/N, donc 0.04 en l'occurrence. Il est possible d'utiliser une option des régressions plus loin pour identifier les valeurs influentes en utilisant d'autres critères. Nous choisissons les 3 options pour pouvoir comprendre les différences entre le modèle sur l'ensemble des observations et le modèle sans les valeurs influentes. 

<center>![](./reg/reg5.png)</center>
<center><small>Figure 53. Faut-il faire les analyses sur l'échantillon complet ou sans les valeurs influentes et faut-il identifier les valeurs influentes ? </small></center>


La Figure 54 permet d'enregistrer les résultats en format Word. Comme précédemment, nous utiliserons plus loin une autre solution. Nous pouvons cocher la case FALSE. 


<center>![](./reg/reg7.png)</center>
<center><small>Figure 54. Faut-il sauvegarder les résultats en MSword ? </small></center>


Lorsqu'on réalise des régressions, on peut faire l'analyse sur les données non centrées ou centrées. Dans un modèle additif, cela a peu d'importance mais dans un modèle avec des interactions (modérations), les effets principaux ne sont plus interprétables si on ne centre pas les variables au préalable. Il est donc recommandé de centrer les variables. Au final, centrer n'est jamais une mauvaise chose mais ne pas le faire peut l'être dans les modèles ayant des interactions entre les variables (Figure 55). 

<center>![](./reg/reg8.png)</center>
<center><small>Figure 55.Faut-il ou non centrer les variables avant de réaliser la régression ? </small></center>


La figure (Figure 56) suivante révèle quelques options qu'il est possible de réaliser en plus grâce à easieR. Vous pouvez faire des modèles hiérarchiques, utiliser des méthodes de sélection (avec plusieurs critères de sélection possible, comme le BIC, le AIC, la valeur du F ou la valeur de la probabilité). Lorsqu'on est dans une démarche exploratoire, il est utile de pouvoir faire une validation croisée et il existe aussi une option pour accéder à d'autres indicateurs pour les mesures d'influence. 

<center>![](./reg/reg9.png)</center>

<center><small>Figure 56. Options disponibles dans easieR pour les régressions </small></center>

Ensuite, l'analyse se réalise. 


### Avec les lignes de commandes

A nouveau, on peut faire des régressions avec easieR à l'aide d'une ligne de commande qui a comme arguments : 

- data : nom du jeu de données. 

-	modele: modèle à tester. il prend la forme de VD~VI1+VI2 ou VD~VI1*VI2.  

-	outlier : caractère indiquant si les analyses doivent être réalisées sur l'ensemble des données, s'il faut identifier les valeurs influentes et s'il faut réaliser l'analyse sans les valeurs influentes. Les valeurs autorisées sont : 'Donnees completes','Identification des valeurs influentes','Donnees sans valeur influente'

-	inf: logique. Faut-il afficher les mesures d'influences.  

-	CV: logique. Faut-il faire une validation croisée (TRUE) ou non (FALSE). 

-	select.m : caractère. Il faut préciser si on veut utiliser des méthodes de sélection. Les choix possibles sont "none" pour aucune,     "Forward - pas-a-pas ascendant" (ou"Forward") pour une méthode pas-à-pas ascendant, "Backward- pas-a-pas descendant" pour une      pas-à-pas descendante (ou "Backward), et "Bidirectionnel" (ou "Both") pour une méthode qui teste tous les modèles possibles.

-	step : faut-il faire une régression hiérarchique. Si oui, il faut une liste avec les variables à tester à chaque étape. 

-	group : caractère correspondant à la ou les variables permettant de réaliser les analyses par groupe. Cet argument n'est disponible qu'en ligne de commande. Elle permet de faire la même régression sur plusieurs sous-groupes. 

- method : si select.m est différent de "none", il s'agit de la méthode utilisée comme critère d'entrée, parmi  "AIC", "F", or "p"

-	criteria : Valeur numérique. Valeur de la probabilité d'entrée dans les méthodes de sélection si la méthode utilisée est "p". 

-	scale : logique, faut-il centrer les variables (TRUE) ou non (FALSE)

-	dial : logique. Faut-il les boîtes de dialogue pour les options ?  

-	info : il s'agit d'un logique qui indique si des messages d'informations doivent être affichés dans la console pour expliquer à quoi correspond les boîtes de dialogue. Par défaut, la valeur est TRUE.

-	sauvegarde : logique indiquant s'il faut une  sauvegarde en fichier msword

-	n.boot : nombre du bootstrap pour les statistiques robustes et pour le posterior des facteurs bayesiens 

-	param :  caractère indiquant quelles analyses doivent être faites. Les valeurs autorisées sont : ‘Test parametrique’,‘Test robustes - impliquant des bootstraps’,‘Facteurs bayesiens’, ou leur version abrégée ‘param’, ‘robustes’, ‘Bayes’


-	rscale: il s'agit du prior si 'Bayes' ou 'Facteurs bayesiens'est sélectionné pour 'param' . Cela correspond approximativement à la valeur de la corrélation qu'on s'attend à avoir. Sa valeur par défaut est 0.353

- html : logique. Indique s'il faut une sortie html. 


Ainsi, pour notre exemple, on peut utiliser la fonction de la manière suivante. 


```{r, echo=T, eval=F}

regressions(data=sex.appeal,
	modele=sex_appeal~charme + humour + intelligence + elegance,
	outlier=c('Donnees completes','Identification des valeurs influentes','Donnees sans valeur influente'),
	inf=FALSE,
	CV=FALSE,
	select.m='none',
	step=NULL,
	group=NULL,
	criteria=0.15,
	scale=TRUE,
	dial=T, 
	info=T,
	sauvegarde=FALSE,
	n.boot=,
	param=c('Test parametrique'),
	rscale=0.353)

```


mais vous pouvez également utiliser les arguments minimum (les données et le modèle) et considérer les options par défaut comme satisfaisantes. 



```{r, echo=T, eval=F}

regressions(data=sex.appeal,
	modele=sex_appeal~charme + humour + intelligence + elegance,
	dial=F,
	outlier='Donnees completes')

```



## 6.2. Interpréter les résultats

```{r, echo=F, eval=T, message=F, warning=F}

reg.out<-regressions(data=sex.appeal,
	modele=sex_appeal~charme + humour + intelligence + elegance,
	outlier=c('Donnees completes','Identification des valeurs influentes','Donnees sans valeur influente'),
	inf=FALSE,
	CV=FALSE,
	select.m='none',
	step=NULL,
	group=NULL,
	criteria=0.15,
	scale=TRUE,
	dial=T, 
	info=T,
	sauvegarde=FALSE,
	n.boot=,
	param=c('Test parametrique'),
	rscale=0.353, html=F)

```




Pour la sortie de résultats, nous commencerons les explications sur la partie identification des valeurs influentes. La partie sans et la partie avec s'interprétant exactement de la même manière. 

Nous observons que deux observations sont considées comme influentes en utilisant comme critère d'influence une distance de Cook de 4/N, ce qui représente 2% de notre échantillon (Tableau 29). 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[2]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 29. Nombre de valeurs influentes considérées comme influentes sur la base d'une distance de Cook de 4/N."
ht

```

Ces deux valeurs sont l'observation 27 et 60. Leur distance de Cook est de 0.05182 et 0.04076 respectivement (Tableau 30). 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[3]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 30. Valeur identifiées comme influentes."
ht

```

L'étape suivante consiste à regarder les statistiques descriptives. A cette étape, on va être attentif au nombre d'observations (est-ce que je fais bien l'analyse sur tout mon échantiillon), au minimum, au maximum (est-ce que ces valeurs sont compatibles avec mes données), à la moyenne et à la médiane (si elles sont proches, c'est que la distribution est symétrique, sinon ce n'est pas le cas) et éventuellement identifier des tendances (mais ce n'est pas quelque chose qu'on peut faire ici)(Tableau 35). 

```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[4]][[1]][[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 35. Statistiques descriptives"
ht

```


L'analyse des graphiques de chacune des variables prises individuelles permet d'avoir une meilleure idée de leur distribution et de la présence de valeurs qui se comporterait de manière étrange. 

```{r echo=F, message=F, warning=F}

#reg.out[[4]][[1]][[2]]


p1<-reg.out[[4]][[1]][[2]][[1]]
p2<-reg.out[[4]][[1]][[2]][[1]]
p3<-reg.out[[4]][[1]][[2]][[1]]
p4<-reg.out[[4]][[1]][[2]][[1]]
p5<-reg.out[[4]][[1]][[2]][[1]]
.multiplot(p1,p2,p3,p4, p5, cols=3)

```

<small>Figure 57. Représentation graphique des différentes variables</small>

Lorsque les données ont été centrées, easieR le précise dans la sortie de résultats. 

```{r echo=F, message=F, warning=F}

reg.out[[4]][[2]]


```


Deux tests de normalité ont été réalisé (Tableau 36). Le premier (et le plus puissant) est le test de Shapiro-Wilk. La probabilité associée à ce test est de 0.125. Elle est supérieure à 0.05,  on tolère l'hypothèse nulle selon laquelle la distribution des données ne se distingue pas d'une distribution normale. Le test de Lilliefors fonctionne sur le même principe : avec une probabilité de 0.117, on tolère l'hypothèse nulle selon laquelle la distribution suit une distribution normale. Ainsi, ici les deux tests aboutissent à la même conclusion. Lorsque les deux tests n'aboutissent pas à la même conclusion, il faut privilégier le test de Shapiro-Wilk lorsque l'échantillon est petit (moins de 50 personnes) car ce test est plus puissant et le test de Lilliefors pour les grands échantillons (plusieurs centaines de personnes). En effet, avec les gros échantillons, de minimes violations de la normalité aboutissent au rejet de l'hypothèse nulle avec le test de Shapiro-Wilk. 



```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[4]][[3]][[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}




caption(ht) <-  "Tableau 36. Tests de normalité."
ht

```

Pour compléter l'examen des tests de normalité, avoir une représentation graphique est toujours utile. En l'occurrence, nous avons la distribution des résidus où on n'a pas réellement l'impression que la distribution est normale (les batonnets gris ne collent pas réellement à la distribution théorique en rouge) et le QQplot ne forme pas une belle ligne droite. Or, c'est cette ligne droite qui indique que la distributioin suit la normalité. 

Donc, ici, même si les tests n'aboutissent pas au rejet d'une distribution normale, il faudrait être prudent pour l'interprétation des résultats et avoir un complément plus robuste aurait été utile. 

```{r echo=F, message=F, warning=F}

p1<-reg.out[[1]][[3]][[2]]
p2<-reg.out[[1]][[3]][[3]]
.multiplot(p1, p2, cols=2)

```

<small>Figure 58. Distribution des résidus en fonction d'une distribution normale théorique et QQplot </small>


Comme il y a débat sur le fait de savoir si c'est le résidu qui doit être distribué normalement ou si c'est l'ensemble des prédicteurs, un test de normalité multivariée est également présenté (Tableau 37). 

**Attention** Au moment de l'écriture de ce document, il y a manifestemment sur la fonction <code>mardia</code> du package 'psych' puisque la probabilité est ici supérieure à 1 pour l'applatissement et l'asymétrie. Ce problème devrait être résolu rapidement après avoir contacté William Revelle pour qu'il corrige le package 'psych'. 



```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[4]][[4]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 37. Test de normalité multivariée (test de Mardia) présentant l'asymétrie et l'aplatissement."
ht

```


Une des conditions d'application des régressions est l'absence de multicolinéarité qu'on peut identifier à l'aide de la tolérance ou du facteur d'inflation de la variance (FIV dans le tableau 38). Les deux sont intimemement liés puisque le facteur d'inflation de la variance est $\frac{1}{tolérance}$. On considère qu'il y a un problème de multicolinéarité quand la tolérance est inférieure à 0.10, et forcément quand le FIV est supérieur à 10.  


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[4]][[5]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 38. Tableau de la tolérance"
ht

```


En l'occurrence, nous sommes à la limite du problème de multicolinérarité puisque le charme et l'humour ont une tolérance à 0.13. 

On peut également faire une analyse visuelle des prédicteurs en réalisant un graphique où le prédicteur est en abcisse et la composante du 'sex_appeal' en ordonées. Si la variable est à une pente de 45%, cela signifie qu'il y a potentiellement un souci (Figure 59). 


```{r echo=F, message=F, warning=F}

reg.out[[4]][[6]]

```

<small>Figure 59. Graphique de chaque prédicteur sur la 'composante' sex appeal </small>


Le dernier indicateur qu'on peut examiner et la valeur propre (eigenvalue en anglais). Si l'indice de la valeur propre est supérieure à 30, cela doit être considéré comme un problème de multicolinéarité (Tableau 39) (Hebbali, 2020). 

A titre personnel, je n'utilise que la tolérance. 




```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[4]][[7]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 39. Tableau des valeurs propres pour identifier la présence d'un problème de multicolinéarité"
ht

```


Il faut également vérifier s'il n'y a pas d'autocorrélation sur les résidus. Cela signifie que les résidus ne doivent pas être corrélés entre eux, sinon le modèle n'explique pas toute la variance possiblement explicable. Le modèle devient alors instable. On peut tester l'absence d'autocorrélation  à l'aide du test de Durbin-Watson (Tableau 40). 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[4]][[8]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 40. Test de Drubin-Watson permettant d'identifier la présence d'autocorrélations"
ht

```

En l'occurrence, la probabilité associée au test de Durbin-Watson est inférieure à 0.05 (0.012). Il faut donc rejeter l'hypothèse nulle d'absence d'autocorrélation. Cette condition d'application n'étant pas respectée, il serait bon soit de supprimer certains prédicteurs de l'analyse, soit d'utiliser des statistiques plus robustes aux violations des conditions d'application. 

LA condition d'application suivante à vérifier est l'homogénéité des variance. On peut analyser cette homogénéité des variances à l'aide du test de Breusch-Pagan (Tableau 41). En l'occurrence, cette condition est respectée puisque la probabilité est de 0.268, donc supérieure à 0.05, nous amenons donc à tolérer l'hypothèse nulle selon laquelle la variance de l'erreur est constante. 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[4]][[9]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 41. Test de Breusch-Pagan testant  la non-constance de la variance d'erreur."
ht

```


A présent que les conditions d'application ont été examinée, nous pouvons regarder nos résultats. Nous pouvons dans un premier temps déterminer si, dans sa globalité, notre modèle explique une part significative de la variance de la variable dépendante (Tableau 42)

En l'occurrence, le modèle explique 78.5% de la variance qui, avec un F(4,93)=84.97, est significatif avec une probabilité associée infiérieure à .001. L'erreur residuelle est la somme des carrés des résidus. Cela permet de recalculer éventuellement la variance de la variable dépendante. 

```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[4]][[10]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 42. Estimation du modèle global."
ht

```


Si le modèle est significatif dans sa globalité, nous ne savons pas encore quels prédicteurs sont significatifs de manière individuelle. Ces informations sont fournies dans le tableau des coefficients de régression, où les coefficients standardisés sont également fournis. 

```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[4]][[11]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 43. Tableau des coefficients de regression."
ht

```


Remarquez que dans le tableau 42, il y a le R², le delta de R² et le R² ajusté. Commençons par le R². Le R² fournit le pourcentage de variance expliqué à chaque étape du modèle (donc d'abord quand il n'y a qu'une variable, ensuite quand il y en a 2...). Ainsi, la différence entre deux étapes permet d'identifier la contribution de chaque variable au modèle en calculant la différence entre les différentes étapes. C'est le delta de R². Cette manière est de faire est acceptable à la condition de ne pas avoir de corrélations (ou des corrélations faibles) entre les différents prédicteurs. 

En ce qui concerne le R² ajusté. Cette valeur devrait être privilégiée quand on présente les résultats d'une régression car la valeur est corrigée pour l'estimation de la variance. Le R² est toujours un peu surestimé. 

Lorsqu'il y a des corrélations entre les différents prédicteurs ou qu'on souhaite être puriste, on peut regarder le tableau de contribution de chaque variable au modèle (Tableau 44). Dans ce tableau, la colonne Zero.order est la corrélation entre la variable indépendante et la variable dépendante, la colonne 'Part' indique de combien le R² va diminuer si on enlève cette variable du modèle et la colonne 'Partial' indique le pourcentage de variance sur la variable dépendante qui n'est pas estimée par les autres variables, mais est spécifiquement estimée par cette variable. (il ne faut pas oublier que ce soit des corrélations, pour passer en pourcentage de variance expliquée, il faut les élever aux carré)

```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(reg.out[[4]][[12]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 44. Contribution de chaque variable au modèle."
ht

```


Enfin, la dernière information fournie est un graphique qui représente le lien entre la variable indépendante et la variable dépendante quand les autres variables sont contrôlées (Figure 60). 

```{r echo=F, message=F, warning=F}

reg.out[[4]][[13]]

```

<small>Figure 60. Lien entre la variable indépendante et la variable dépendante quand les autres variables sont contrôlées </small>


## 6.3. Exercices {.tabset .tabset-fade .tabset-pills}

### Enoncés

**Enoncé 1**
En reprenant l'exercice "poids", faites une régression avec les boîtes de dialogue. En l'occurrence, on veut prédire les divorces par la prise de poids et la consommation de margarine. 

**Enoncé 2** 

En utilisant, la ligne de commande et en précisant uniquement, le jeu de données et le modèle, réalisez un modèle de régression sur le jeu de données utilisé dans l'exemple sur le Détresse. (on prédit la détresse par le coping en contrôlant l'âge)


### Solutions 

**Enoncé 1**

```{r, echo=F, eval=T}
import(file='Exercices.Bruxelles.xlsx',
       dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
       type='Fichier Excel',
       dec='.',
       sep=';',
       na.strings='NA',
       sheet='poids',
       name='poids')
```

```{r, echo=T, eval=T}


reg.out<-regressions(data=poids,
	modele=divorces~margarine+prise_poids_10ans,
	outlier=c('Donnees completes'), html=F)




```






**Enoncé 2** 

```{r, echo=T, eval=F}

reg.out<-regressions(data=Detresse,
	modele=detresse~coping + age, html=F) 

```


# 7. Les t de Student 

Pour les t de Student, il existe trois variante du test : le t de Student comparaison à une norme, le t de Student pour échantillon appariés et le t de Student pour échantillon indépendants. 

## 7.1. Les t de Student pour échantillons indépendants 

Pour illustrer le t de Student pour échantillons indépendants,  je me suis posé une question fondamentale que tout le monde se pose mais pour laquelle personne n’a jamais cherché la réponse : pourquoi les super héros gagnent-ils toujours à la fin ?

Pour tester cette hypothèse, nous avons réalisé à l’aide de méthodes complexes des mesures de la force des super-héros et des super-vilains. Les données sont dans la feuille ‘super.heros’ (Tableau 45.

```{r, echo=F}

import(file='Exercices.Bruxelles.xlsx',
	dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
	type='Fichier Excel',dec='.',sep=';',na.strings='NA',sheet='super.heros', name='SH')

datatable(SH, caption="Tableau 45. Jeu de données 'super-héros'")

```

### 7.1.1. Réaliser un t de Student pour échantillons indépendants{.tabset .tabset-fade .tabset-pills}

#### Avec les boîtes de dialogue 

On peut directement accéder au t de Student avec la fonction <code>test.t()</code>. La première boîte de dialogue qui apparaît est celle qui permet de choisir le type de t de Student que nous souhaitons réaliser (Figure 61). En l'occurrence, nous allons choisir le t de Student pour échantillons indépendant. 

<center>![](./t/independent/ind1.png)</center>

<small>Figure 61. Type de t de Student souhaité. </small>

Ensuite, il faut choisir le jeu de données (SH, Figure 62). 

<center>![](./t/independent/ind2.png)</center>

<small>Figure 62. Choix du jeu de données. </small>

L'étape suivante consiste à choisir la variable dépendante (Figure 63,  gauche) et indépendante (Figure 63, droite). La variable dépendante doit être numérique et la variable indépendante doit être qualitative avec deux modalités. Il est donc nécessaire de s'en être assuré lors de l'importation des données. 

<center>![](./t/independent/ind3.png)![](./t/independent/ind4.png)</center>

<small>Figure 63. Choix de la variable dépendante (gauche) et indépendante (droite). </small>

Il faut ensuite déterminer quelles analyses sont souhaitées. Le test paramétrique est le test de Student, le non paramétrique est le test de Mann-Whitney (connu aussi sous le nom de test de Wilcox), les tests robustes vont proposer des boostraps et des analyses sur des indicateurs plus robustes (comme la médiane ou la moyenne tronquée). Enfin, les facteurs bayesiens permettent d'obtenir les analyses sous un angle bayesien. En l'occurrence, nous choisirons les statistiques paramétrriques et non paramétriques (Figure 64). 

<center>![](./t/independent/ind5.png)</center>
<small>Figure 64. Type d'analyse qui devra être réalisé entre les statistiques paramétriques, non paramétrique, robustes et les facteurs bayesiens</small>


On continue en déterminant si on souhaite que l'analyse soit réalisée sur l'échantillon complet, s'il faut identifier la présence de valeurs influentes et/ou si on veut l'analyse sur l'échantilon dont on a supprimé les valeurs influentes. Par défaut, les valeurs influentes sont identifiées et supprimées (Figure 65).

La plupart du temps, il est préférable d'analyser les données sans les valeurs influentes. Néanmoins, il y a des exceptions : par exemple, quand on travaille avec une population pathologique, il est normal de s'attendre à des valeurs influentes. Par ailleurs, certains chercheurs préférent laisser toutes les observations ou utiliser une autre alternative (e.g., les tests non paramétriques) que de supprimer les valeurs influentes. Une attitude minimale est d'aller identifier s'il y a des valeurs influentes. Par défaut, c'est le test de Grubbs qui est utilisé pour identifier la présence des valeurs influentes. Entre l'analyse "sans les valeurs influentes" et "avec les valeurs influentes", il est préférable de choisir "sans" dans la majorité des cas. Néanmoins, je trouve toujours intéressant de regarder si l'interprétation change lorsque les valeurs influentes sont présentes ou non, peu importe le sens. En effet, cela donne une appréciation de la robustesse de l'effet, et permet de s'assurer que la présence/absence de l'effet ne dépend pas que de quelques observations, c'est pourquoi il est préférable de laisser tout coché dans la Figure 65.



<center>![](./t/independent/ind6.png)</center>

<small>Figure 65. Sur quel échantillon les analyses doivent-elles être réalisées ? L'échantillon complet ou sans les valeurs influentes. Faut-il identifier ces valeurs influentes.</small>

La dernière boîte de dialogue permet de sauvegarder les résultats en word. Cette option n'est pas compatible avec les utilisateurs macOS. Par défaut, nous ne le feront pas (parce qu'il existe une autre option qui sera bien plus efficace pour sauvegarder les analyses que nous avons réalisées)(Figure 66).


<center>![](./t/independent/ind7.png)</center>
<small>Figure 66. Faut-il enregistrer les résultats en MSword. En l'occurence, le choix est négatif.</small>


#### Avec les lignes de commandes

La fonction <code>test.t</code> permet de réaliser un t de Student pour échantillons indépendants en ligne de commande. Les arguments de cette fonction sont :


- X : caractère qui correspond au nom de la variable ou des variables dépendantes

- Y: caractère qui correspond au nom de la variable  indépendante.

- group :  caractère qui correspond au nom de la variable qualitative pour laquelle l'analyse doit être faite en sous-groupes. Cette option ne s'applique que pour le t de Student comparaison à une norme. 

- choix : caractère. Il faut préciser le type de t de Student souhaité parmi : 'Deux echantillons independants', 'Deux echantillons appariés', et 'Comparaison a une norme".

- sauvegarde :  logique indiquant s'il faut une  sauvegarde en fichier msword

- outlier : caractère indiquant si les analyses doivent être réalisées sur l'ensemble des données, s'il faut identifier les valeurs influentes et s'il faut réaliser l'analyse sans les valeurs influentes. Les valeurs autorisées sont : 'Donnees completes','Identification des valeurs influentes','Donnees sans valeur influente'

- z : notez qu'il s'agit d'un z miniscule par opposition du z majuscule présenté précédemment. Pour supprimer les valeurs influente, on peut utiliser le test de Grubbs (par défaut) ou le score z. Si z vaut NULL, le test de Grubbs sera utilisé. Si le z a une valeur numérique, cette valeur numérique sera utilisée pour déterminer l'éloignement en écart-type à partir duquel le résidu standardisé d'une observation va être considéré comme influent. Les valeurs habituelles sont : 1.96 (5%), 2.56 (1%), 3.26 (0.1%). 

- data : nom du jeu de données ; 

- alternative : Caractères indiquant si l'analyse doit être réalisé de manière unilatérale ou bilatérale. LEs choix autorisés sont : 'two.sided', , "lower", "two.sided". Cet argument n'est pas proposé en boîte de dialogue. 

- mu : valeur numérique indiquant la valeur de la norme qui sert de référence dans le t de Student comparaison à une norme. 

- formula : les variables peuvent également spécifiées sous la forme d'une forme VD~VI

- n.boot : nombre du bootstrap pour les statistiques robustes et pour le posterior des facteurs bayesiens 

- param : caractère indiquant quelles analyses doivent être faites. Les valeurs autorisées sont : 'Test parametrique','Test non parametrique','Test robustes - impliquant des bootstraps','Facteurs bayesiens', ou leur version abrégée 'param', 'non param', 'robustes', 'Bayes'

- info : il s'agit d'un logique qui indique si des messages d'informations doivent être affichés dans la console pour expliquer à quoi correspond les boîtes de dialogue. Par défaut, la valeur est TRUE. 

- rscale : il s'agit du prior si 'Bayes' ou 'Facteurs bayesiens'est sélectionné pour 'param' . 

- html : logique. Indique s'il faut une sortie html. 


```{r, echo=T, eval=F, warning=F, message=F}

test.t(X=c('Force'), Y='Groupe',group=NULL, 
	choix='Deux echantillons independants', sauvegarde = FALSE,
	outlier=c('Donnees completes','Identification des valeurs influentes',
	'Donnees sans valeur influente'),z=NULL, data=SH,
	alternative='two.sided', mu=NULL,formula =NULL, n.boot=NULL,
	param=c('Test parametrique','Test non parametrique'),info=T,
	 rscale=0.707, hmtl=F)
```


## 7.1.2. Interpréter les résultats

```{r, echo=F, eval=T, warning=F, message=F}

t.out<-test.t(X=c('Force'), Y='Groupe',group=NULL, 
	choix='Deux echantillons independants', sauvegarde = FALSE,
	outlier=c('Donnees completes','Identification des valeurs influentes',
	'Donnees sans valeur influente'),z=NULL, data=SH,
	alternative='two.sided', mu=NULL,formula =NULL, n.boot=NULL,
	param=c('Test parametrique','Test non parametrique'),info=T,
	 rscale=0.707, html=F)
```

Pour la sortie de résultats, nous commencerons les explications sur la partie identification des valeurs influentes. La partie sans et la partie avec s'interprétant exactement de la même manière. 


Dans le Tableau 46, nous avons les résultats du test de Grubbs dont l'hypothèse nulle est qu'il n'y a pas de valeurs influentes. En l'occurrence, comme la probabilité est de 0.5709, nous tolérons l'hypothèse nulle selon laquelle il n'y a pas de valeur influente dans l'échantillon. Remarquezz dans la sortie de résultats que, quand il n'y a pas de valeurs influentes, easieR ne refait pas l'analyse et ne présente les données que sur l'échantillon complet. 

```{r echo=F, message=F, warning=F}
library(huxtable)
round.ps<-function (x) { substr(as.character(ifelse(x < 0.0001, ' <.0001', 
                                                                ifelse(round(x, 2) == 1, ' >.99', 
                                                                formatC(x, digits = 4, format = 'f')))), 2, 7)}
myf<-function(x){which(x<0.05)}
tableau<-as.data.frame(t.out[[1]][[2]][[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 46. Test de Grubbs testant l'absence de valeurs influentes."
ht

```


En l'occurrence, la valeur qui s'éloigne le plus de la moyenne de chaque groupe 19.94 unités, mais ce n'est pas suffisamment important pour considérer que c'est une valeur influente. 

```{r echo=F, message=F, warning=F}

t.out[[1]][[2]][[2]]


```

Il s'ensuit qu'aucune observation n'est retirée de l'échantillon (Tableau 47)





```{r echo=F, message=F, warning=F}
library(huxtable)
round.ps<-function (x) { substr(as.character(ifelse(x < 0.0001, ' <.0001', 
                                                                ifelse(round(x, 2) == 1, ' >.99', 
                                                                formatC(x, digits = 4, format = 'f')))), 2, 7)}
myf<-function(x){which(x<0.05)}
tableau<-as.data.frame(t.out[[1]][[2]][[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 47. Aucune observation n'est considérée comme influente."
ht

```


L'étape suivante consiste à regarder les statistiques descriptives. A cette étape, on va être attentif au nombre d'observations (est-ce que je fais bien l'analyse sur tout mon échantiillon), au minimum, au maximum (est-ce que ces valeurs sont compatibles avec mes données), à la moyenne et à la médiane (si elles sont proches, c'est que la distribution est symétrique, sinon ce n'est pas le cas) et éventuellement identifier des tendances (Tableau 48). En l'occurrence, on identifie que les deux moyennes sont plutôt proches.  


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(t.out[[1]][[1]][[1]][[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 48. Tableau des statistiques descriptives."
ht

```


On peut compléter l'analyse descriptive par l'analyse du graphique qui est un plot violon. Celui-ci montre les moyennes et distribution pour les deux groupes. Une rotation de la tête permet de savoir si la distribution suit approximativement une distribution normale (Figure 67). 


```{r echo=F, message=F, warning=F}
t.out[[1]][[1]][[1]][[2]]
```

<small>Figure 67. Graphique violon montrant la moyenne, l'écart-type et la distribution des observations.</small>


Deux tests de normalité ont été réalisé. Le premier (et le plus puissant) est le test de Shapiro-Wilk. La probabilité associée à ce test est de 0.40 (Tableau 49). Elle est supérieure à 0.05,  on tolère l'hypothèse nulle selon laquelle la distribution des données ne se distingue pas d'une distribution normale. Le test de Lilliefors fonctionne sur le même principe : avec une probabilité de 0.40, on tolère l'hypothèse nulle selon laquelle la distribution suit une distribution normale. Ainsi, ici les deux tests aboutissent à la même conclusion. Lorsque les deux tests n'aboutissent pas à la même conclusion, il faut privilégier le test de Shapiro-Wilk lorsque l'échantillon est petit (moins de 50 personnes) car ce test est plus puissant et le test de Lilliefors pour les grands échantillons (plusieurs centaines de personnes). En effet, avec les gros échantillons, de minimes violations de la normalité aboutissent au rejet de l'hypothèse nulle avec le test de Shapiro-Wilk.

```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(t.out[[1]][[1]][[2]][[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 49. Tests de Shapiro-Wilk et de Lilliefors pour tester la normalité."
ht

```


Pour compléter l'examen des tests de normalité (Figure 68), avoir une représentation graphique est toujours utile. En l'occurrence, nous avons la distribution des résidus où on n'a pas réellement l'impression que la distribution est normale (les batonnets gris ne collent pas réellement à la distribution théorique en rouge) et le QQplot ne forme pas une belle ligne droite. Or, c'est cette ligne droite qui indique que la distributioin suit la normalité. 


```{r echo=F, message=F, warning=F}

p1<-t.out[[1]][[1]][[2]][[2]]
p2<-t.out[[1]][[1]][[2]][[3]]
.multiplot(p1, p2, cols=2)

```

<small>Figure 68. Distribution des résidus en fonction d'une distribution normale théorique et QQplot </small>

La seconde condition d'application à devoir respecter est l'homogénéité des variances qu'on teste avec le test de Levene (Tableau 50).En l'occurrence, avec un probabilité de 0.65, on tolère l'hypothèse nulle selon laquelle les deux variances ne se distinguent pas. Ainsi, notre condition d'homogénéité des variances est respectée. 

```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(t.out[[1]][[1]][[3]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 50. Tests de Levene pour vérifier l'homogénéité des variances."
ht

```


On peut à présent s'intéresser aux résultats de l'analyse principale (Tableau 51). Dans ce tableau, on retrouve la valeur du t (-0.445), les degrés de liberté pour le test (30), la valeur de la probabilité associée au test (0.659), l'intervalle de confiance autour de l'estimation du t et deux mesures de taille d'effet : le d de Cohen et le R² obtenu par le calcul de la corrélation biserial. Il est à noter qu'il y a une ligne sans la correction de Welch et une ligne avec cette correction. La correction de Welch est simplement une correction des degrés de liberté (identifiez que les degrés de liberté ont une virgule quand il y a la correction) qu'on applique pour corriger l'hétérogénéité des variances. Fondamment, on peut toujours appliquer cette correction. Il serait donc préférable de toujours privilégier la seconde ligne.  

```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(t.out[[1]][[1]][[4]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 51. t de Student."
ht

```

La Figure 69 est un représentation graphique assez classique d'un t de Student pour échantillon indépendant : une représentation en batonnet qui représente la moyenne et la barre d'erreur qui représente l'écart-type. 

```{r echo=F, message=F, warning=F}

t.out[[1]][[1]][[5]]


```

<small>Figure 69. Représentation graphique du t de Student sous la forme de batonnet. La barre d'erreur est l'écart-type </small>



La dernière analyse représente l'équivalent non paramétrique du t de Student pour échantillon indépendant, à savoir le test de Mann-Whitney (Tableau 52), aussi appelé test de Wilcoxon. Outre la valeur du test de Wilcoxon, et la probabilité associé, on retrouve deux mesures de taille d'effet pour les tests non paramétrique : un z et un r. En ayant ces mesures de taille d'effet, il s'agit d'une raison supplémentaire de ne pas négliger les tests non paramétriques lorsque les conditions d'applications ne sont pas respectées (voire quand on sait par avance qu'elles ne le seront pas, comme quand on traite des variables ordinales). 

```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(t.out[[1]][[1]][[4]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 52. Test de Mann-Whitney, également appelé test de Wilcoxon."
ht

```

Tant l'analyse statistique que l'analyse visuelle, nous amène à constater que, si les super-héros gagnent toujours à la fin, ce n'est pas parce qu'ils sont plus forts que les super-vilains. Nous devrons donc tester d'autres hypothèses pour comprendre ce phénomène. 


## 7.2. Les t de Student pour échantillons appariés


Une étude a montré que les enfants qui avaient subi des châtiments corporels pendant 5 ans étaient plus déprimés que les enfants qui n'avaient pas subi de tels châtiments. Quelque peu embêté par un problème méthodologique, nous avons refait l'étude en mesurant le niveau de dépression d'enfants avant l'expérimentation. Ensuite, pendant 5 ans, nous les avons soumis à des châtiments corporels et nous avons mesuré à nouveau leur niveau de dépression après cette période de 5 ans. (Cette étude est inspirée de Turnet & Muller, 2004)

Les données pour cet énoncé sont dans la feuille battre et son présentées dans le Tableau 53. 

```{r, echo=F}

import(file='Exercices.Bruxelles.xlsx',
	dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
	type='Fichier Excel',dec='.',sep=';',na.strings='NA',sheet='Battre',name='battre')

datatable(battre, caption="Tableau 53. Jeu de données 'Battre'")

```

	
	

### 7.2.1. Réaliser un t de Student pour échantillons appariés{.tabset .tabset-fade .tabset-pills}

#### Avec les boîtes de dialogue 

On peut directement accéder au t de Student avec la fonction <code>test.t()</code>. La première boîte de dialogue qui apparaît est celle qui permet de choisir le type de t de Student que nous souhaitons réaliser (Figure 70). En l'occurrence, nous allons choisir le t de Student pour échantillons appariés 

<center>![](./t/paired/paired1.png)</center>

<small>Figure 70. Type de t de Student souhaité. </small>

Ensuite, il faut choisir le jeu de données (battre, Figure 71). 

<center>![](./t/paired/paired2.png)</center>

<small>Figure 71. Choix du jeu de données. </small>


A cette étape, il est possible de faire un t de Student pour échantillons appariés en utilisant deux formats différents. Le premier format est la format qu'on vous a probablement appris : une ligne correspond à une observation et les différentes mesures se trouvent dans des colonnes différentes sur la même ligne. C'est le format qu'on a dans le Tableau 53. On pourrait également avoir un autre format de données, appelé un format long. Dans ce format long, il y a une variable qui identifie les participants. Cet identifiant sera répété deux fois (une fois pour chaque modalité de la variable indépendante), une colonne pour la variable indépendante (qui aura donc deux modalités) et une colonne pour la variable dépendante. Ainsi, notre jeu de données en format long prendre la forme du Tableau 54.

```{r, echo=F, eval=T, warning=F, message=F, fig.show='hide'}

t.out<-test.t(X=c('Avant_les_chatiments_'), Y='Apres_5_ans_de_chatiments',group=NULL, 
		choix='Deux echantillons apparies',
	 sauvegarde = FALSE,outlier=c('Donnees completes','Identification des valeurs influentes',
	'Donnees sans valeur influente'),z=NULL, 
	data=battre,alternative='two.sided', mu=NULL,formula =NULL, 
	n.boot=NULL,param=c('Test parametrique','Test non parametrique'),info=T, rscale=0.707, html=F)

DT::datatable(battre.format.long, caption="Tableau 54. Jeu de données 'battre' en format long")



```


Donc, en l'occurrence, notre format de données est présenté en format large. 


<center>![](./t/paired/paired3.png)</center>
<small>Figure 72. Format du jeu de données. </small>


L'étape suivante consiste à choisir la variable au temps 1 (Figure 73,  gauche) et au temps 2 (Figure 73, droite). Ici les variables doivent être numériques, correspondant la variable mesurée à chacun de temps. Il est donc nécessaire de s'être assuré du format de présentation des données lors de l'importation des données. 

**Note** Si on avait eu une présentation en format long, il aurait fallu une variable dépendante et indépendante de manière tout à fait identique à ce qu'on a dans le t de Student pour échantillon indpépendant. 

<center> ![](./t/paired/paired5.png) ![](./t/paired/paired6.png) </center>


<small>Figure 73. Choix de la variable au temps 1 (gauche) et au temps 2 (droite). </small>


Puisque nous avions un format large, easieR va transformer les données en un format long. Il y a deux raisons fondamentales à cela. La première est que le format long est plus logique (cela sera d'autant plus développé dans le cas de l'analyse de variance à mesure répétée). La seconde est que cette transformation est requise pour réaliser les graphiques. Il faut donc donner un nom aux variables. Ces noms doivent non seulement être explicites et serviront pour les graphiques, il est donc opportun de choisir des noms qui pourront) servir dans la graphique final (Figure 74). En l'occurrence, la variable indépendante peut s'appeler 'moment'. Cela correspond bien à notre exemple et la variable dépendante sera appelée 'dépression'. 

<center> ![](./t/paired/paired7.png){width=45%} ![](./t/paired/paired8.png){width=45%} </center>


<small>Figure 74. Nom à donner pour la variables indépendante (gauche) et dépendante (droite) pour le format long. </small>


Il faut ensuite déterminer quelles analyses sont souhaitées. Le test paramétrique est le test de Student, le non paramétrique est le test de Mann-Whitney (connu aussi sous le nom de test de Wilcox), les tests robustes vont proposer des boostraps et des analyses sur des indicateurs plus robustes (comme la médiane ou la moyenne tronquée). Enfin, les facteurs bayesiens permettent d'obtenir les analyses sous un angle bayesien. En l'occurrence, nous choisirons les statistiques paramétrriques et non paramétriques (Figure 7). 

<center>![](./t/paired/paired9.png)</center>


<small>Figure 75. Type d'analyse qui devra être réalisé entre les statistiques paramétriques, non paramétrique, robustes et les facteurs bayesiens</small>


On continue en déterminant si on souhaite que l'analyse soit réalisée sur l'échantillon complet, s'il faut identifier la présence de valeurs influentes et/ou si on veut l'analyse sur l'échantilon dont on a supprimé les valeurs influentes. Par défaut, les valeurs influentes sont identifiées et supprimées (Figure 76).

La plupart du temps, il est préférable d'analyser les données sans les valeurs influentes. Néanmoins, il y a des exceptions : par exemple, quand on travaille avec une population pathologique, il est normal de s'attendre à des valeurs influentes. Par ailleurs, certains chercheurs préférent laisser toutes les observations ou utiliser une autre alternative (e.g., les tests non paramétriques) que de supprimer les valeurs influentes. Une attitude minimale est d'aller identifier s'il y a des valeurs influentes. Par défaut, c'est le test de Grubbs qui est utilisé pour identifier la présence des valeurs influentes. Entre l'analyse "sans les valeurs influentes" et "avec les valeurs influentes", il est préférable de choisir "sans" dans la majorité des cas. Néanmoins, je trouve toujours intéressant de regarder si l'interprétation change lorsque les valeurs influentes sont présentes ou non, peu importe le sens. En effet, cela donne une appréciation de la robustesse de l'effet, et permet de s'assurer que la présence/absence de l'effet ne dépend pas que de quelques observations, c'est pourquoi il est préférable de laisser tout coché dans la Figure 65.



<center>![](./t/paired/paired10.png)</center>

<small>Figure 76. Sur quel échantillon les analyses doivent-elles être réalisées ? L'échantillon complet ou sans les valeurs influentes. Faut-il identifier ces valeurs influentes.</small>


La dernière boîte de dialogue permet de sauvegarder les résultats en word. Cette option n'est pas compatible avec les utilisateurs macOS. Par défaut, nous ne le feront pas (parce qu'il existe une autre option qui sera bien plus efficace pour sauvegarder les analyses que nous avons réalisées).


<center>![](./t/paired/paired11.png)</center>
<small>Figure 77. Faut-il enregistrer les résultats en MSword.</small>


#### Avec les lignes de commandes

La fonction <code>test.t</code> ayant déjà été décrite pour le t de Student pour échantillons indépendant, nous ne la réexpliquerons pas. La seule différence est que tant X et Y peuvent être des valeurs numériques dans ce cas. 

Remarquez qu'un des avantages de easieR est qu'on peut présenter les données en format large ou en format long. Comparez les deux formules ci-dessous. Dans le premier cas, on utilise les données en format large (directement importées du fichier excel) alors que dans le second cas, on utilise les données en format long (données qui ont été transformées intrinsinquement par easieR et rajouter à la mémoire de R).

```{r, echo=T, eval=F, warning=F, message=F}

test.t(X=c('Avant_les_chatiments_'), Y='Apres_5_ans_de_chatiments',group=NULL, 
		choix='Deux echantillons apparies',
	 sauvegarde = FALSE,outlier=c('Donnees completes','Identification des valeurs influentes',
	'Donnees sans valeur influente'),z=NULL, 
	data=battre,alternative='two.sided', mu=NULL,formula =NULL, 
	n.boot=NULL,param=c('Test parametrique','Test non parametrique'),info=T, rscale=0.707, html=F)




test.t(X=c('Depression'), Y='Moment',group=NULL, choix='Deux echantillons apparies',
	 sauvegarde = FALSE,outlier=c('Donnees completes','Identification des valeurs influentes',
	'Donnees sans valeur influente'),z=NULL, 
	data=battre.format.long,alternative='two.sided', mu=NULL,formula =NULL, 
	n.boot=NULL,param=c('Test parametrique','Test non parametrique'),info=T, rscale=0.707)
```


## 7.2.2. Interpréter les résultats

Les sorties de résultats pour le t de Student pour échantillons appariés sont sensiblement les mêmes que celles du t de Student pour échantillons indépendants. Il y a néanmoins quelques différences notables. 

La première différence notable est qu'on ne doit pas tester l'homogénéité des variances. Cela se justifie par le fait que le calcul du test de Student pour échantillon apparié consiste à faire la différence entre les deux moments et à calculer un t de Student de comparaison à une norme où la norme vaut 0. Finalement, on pourrait formuler l'hypothèse nulle du t de Student pour échantillons appariés de la manière suivante : la moyenne des différences entre les deux moments vaut 0. L'hypothèse alternative serait alors que la moyenne des différences entre les deux moments est différente de 0. 

La seconde différence notable est liée à la représentation graphique du t de Student pour échantillons appariés. Dans la Figure 78, l'estimation de la variance se fait sur le calcul de l'écart-type pour chacun des temps de mesure. Cela a deux conséquences illogiques. La première conséquence illogique est que les variances peuvent être différentes entre T0 et T1 alors que ce sont les mêmes individus. La seconde conséquence, liée à la première est que l'écart-type de chaque moment n'informe en rien sur les inférences qu'on peut faire puisque l'écart-type utilisé dans le t de Student est calculé sur la base des différences entre les deux moments. Bien que cette présentation soit illogique, elle représente la représentation classique des t de Student pour échantillon appariés. 


```{r, echo=F, eval=T, warning=F, message=F, fig.show='hide'}

t.out<-test.t(X=c('Avant_les_chatiments_'), Y='Apres_5_ans_de_chatiments',group=NULL, 
		choix='Deux echantillons apparies',
	 sauvegarde = FALSE,outlier=c('Donnees completes','Identification des valeurs influentes',
	'Donnees sans valeur influente'),z=NULL, 
	data=battre,alternative='two.sided', mu=NULL,formula =NULL, 
	n.boot=NULL,param=c('Test parametrique','Test non parametrique'),info=T, rscale=0.707, html=F)

```


```{r, echo=F, eval=T, warning=F, message=F}
t.out[[1]][[1]][[4]]
```

<small>Figure 78. Représentation classique des t de Student pour échantillons appariés.</small>

On peut néanmoins pallier à ces deux soucis en adoptant la procédure décrite par Loftus et Masson (1994) où on harmonise les écart-types sur les deux temps de mesure pour que cela colle à l'écart-type des différences (Figure 79).

```{r, echo=F, eval=T, warning=F, message=F}
t.out[[1]][[1]][[5]]


```

<small>Figure 78. Représentation ajustée des t de Student pour échantillons appariés en appliquant la procédure de Loftus et Masson (1994).</small>

## 7.3. Exercices {.tabset .tabset-fade .tabset-pills}

### Enoncés

**Enoncé 1**

Des chercheurs (2011) se sont demandé si le TOLD-P:3 permettait de discriminer des enfants présentant différentes formes de dysphasies. Ils ont donc comparé les scores obtenus par 50 enfants présentant une dysphasie phonologico-syntaxique (PS, les enfants présentent des difficultés pour s’exprimer) aux scores obtenus par 50 enfants présentant une dysphasie sémantique pragmatique (SP, les enfants présentent des troubles lexicaux) à ce tests. Quels résultats obtiennent-ils ? 

Utilisez la fonction en ligne de commande en utilisant l'argument 'formula' plutôt que X et Y. 

Les données sont dans la feuille TOLD

```{r, echo=F}

import(file='Exercices.Bruxelles.xlsx',
	dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
	type='Fichier Excel',dec='.',sep=';',na.strings='NA',sheet='TOLD',name='TOLD')

datatable(TOLD, caption="Tableau 54. Jeu de données 'TOLD'")

```


**Enoncé 2**

Un chercheur s’intéresse au développement de la métamémoire chez les enfants. Il raconte à chaque enfant 20 histoires qui se terminent par une question. Voici un exemple de ces histoires : « Voici un sablier. Un sablier sert à mesurer le temps. Si tu dois apprendre le prénom de nouveaux enfants dans ta classe, est-ce que ce sera plus facile avec ou sans le sablier ? » Toutes les histoires sont construites sur le même schéma. Est-ce que les capacités de métamémoire des enfants évoluent entre 4 et 5 ans ? 

Comment savez-vous s'il s'agit d'un t de Student pour échantillon indépendant ou pour échantillon apparié ? Faites l'analyse en ligne de commande. Si des boîtes de dialogue apparaissent, identifiez l'argument sur lequel vous vous êtes trompé. 

Les données sont dans la feuille metamémoire. 

```{r, echo=F}

import(file='Exercices.Bruxelles.xlsx',
	dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
	type='Fichier Excel',dec='.',sep=';',na.strings='NA',sheet='metamemoire',name='metamemoire')

datatable(battre, caption="Tableau 55. Jeu de données 'metamemoire'")

```


### Solutions 


**Enoncé 1**


```{r, echo=T, eval=T, warning=F, message=F}
test.t(group=NULL, 
	choix='Deux echantillons independants', sauvegarde = FALSE,
	outlier=c('Donnees completes','Identification des valeurs influentes',
	'Donnees sans valeur influente'),z=NULL, data=TOLD,
	alternative='two.sided', mu=NULL,formula =TOLD~groupe, n.boot=NULL,
	param=c('Test parametrique','Test non parametrique'),info=T,
	 rscale=0.707, html=F)

```

**Enoncé 2**

```{r, echo=T, eval=T, warning=F, message=F}
test.t(X="X4ans", Y="X5ans", group=NULL, 
	choix='Deux echantillons apparies', sauvegarde = FALSE,
	outlier=c('Donnees completes','Identification des valeurs influentes',
	'Donnees sans valeur influente'),z=NULL, data=metamemoire,
	alternative='two.sided', mu=NULL, n.boot=NULL,
	param=c('Test parametrique','Test non parametrique'),info=T,
	 rscale=0.707, html=F)

```


# 8. Fiabilité de la mesure 

Pour illustrer les mesures de fiabilité, nous reprendronsle jeu de données sur les difficultés en statistiques que nous avons utilisé pour les matrices de corrélations. 
Nous aimerions savoir si les questions de ce questionnaires sont suffisamment fiables pour mesurer les difficultés en statistiques (Tableau 56). 


```{r, echo=F}
 

datatable(diff.stat, caption="Tableau 56. Jeu de données 'Difficultés en statistiques'")

```

## 8.1. Réaliser des mesures de fiabilité{.tabset .tabset-fade .tabset-pills}

#### Avec les boîtes de dialogue

Pour réaliser des analyses de fiabilité, on peut utiliser directement la fonction <code>fiabilite()</code>. A partir de cette fonction, on accède à une boîte de dialogue qui permet de décider quelle mesure de fiabilité est souhaitée. En l'occurrence, nous allons réaliser un alpha de Cronbach (Figure 79. 

<center>![](./fiab/fiab1.png)</center>
<small>Figure 79. Choix de l'analyse de fiabilité souhaitée. En l'occurrence, il s'agit un alpha de Cronbach.</small>


Comme précédemment pour les matrices de corrélations, les étapes suivantes consistent à choisir le jeu de données et les variables sur lesquelles l'analyse doit être faites (Figure 80). Nous allons utiliser le jeu de données 'diff.stats' et on fera l'alpha de Cronbach sur toutes les variables. 


<center> ![](./fiab/fiab2.png) ![](./fiab/fiab3.png) </center>
<small>Figure 80. Choix du jeu de données (diff.stats) et des variables à analyser.</small>

Comme pour l'analyse complète, la question qui se pose est de savoir si on fait la matrice de corrélations sur les données complètes ou sur les données pour lesquelles on a enlevé les valeurs influentes. Les valeurs influentes sont déterminées sur la base de la distance de Mahalanobis (Figure 81). 


<center>![](./fiab/fiab5.png)</center>
<small>Figure 81. Donnnées sur lesquelles l'analyse doit être réalisée : données complètes ou sans valeurs influentes.</small>


Dans la Figure 82, vous devez spécifier la nature des variables dans le jeu de données. Ce choix impactera le type de corrélations utilisées. Pour les variables dichotomiques/ordinales, ce sera des corrélations tetrachoriques/polychoriques qui seront utilisées, tandis que ce seront des corrélations de Bravais-Pearson pour les variables quantitatives. Lorsque les données contiennent les deux types de variables, les corrélations seront des corrélations tétrachoriques/polychoriques entre les variables dichotomiques et ordinales, des corrélations de Bravais-Pearson entre les variables quantitatives et des corrélation bisériales et polysériales entre les variables quantitatives et les variables dichotomiques/ordinales. 
Dans ce cas, si une variable a moins de 9 modalités, elles est considérée comme ordinale, sinon elle est considérée comme quantitative. 
La raison de l'utilisation de ces matrices de corrélations moins connues est que, non seulement statistiquement, il est plus pertinent de les utiliser, mais surtout cela va améliorer la qualité de l'estimation. 

<center>![](./fiab/fiab6.png)</center>
<small>Figure 82. Préciser le type de variables du jeu de données permet de spécifier en interne le type de corrélations qui seront réalisées.</small>

Lorsqu'on réalise un alpha de Cronbach, il est nécessaire que les corrélations entre les variables soient positives. Elles peuvent être négatives lorsque les items sont inversés. Pour comprendre les items inversés, imaginons un questionnaire pour lequel on veut mesurer la dépression, et on a des items du type 'je me sens triste'. Si un item est 'habituellement, je me sens heureux', l'item est inversé par rapport aux autres items. 

Il est possible de spécifier dans easieR quels sont les items qui ont été inversés. Si ce n'est pas précisé, easieR (via la fonction <code>alpha</code> du package 'psych') va considérer que les variables qui ont des corrélations négatives avec les autres variables sont des items inversés. En l'occurrence, nous ne préciserons pas quels items sont inversés (Figure 83). Si nous avions indiqué qu'il y avait des items inversés, une nouvelle boîte de dialogue serait apparue dans laquelle il aurait fallu préciser les items inversés. 

**Note** Ici, on aurait pu considérer certains items comme étant inversés, notamment avec l'item 3 qui est 'Les écart-types, ça m’excite'. Cet item, en terme de contenu va dans le sens inverse des autres items. 

<center>![](./fiab/fiab7.png)</center>
<small>Figure 83. Y a-t-il des items qui ont été inversés ?</small>

La dernière boîte de dialogue permet de sauvegarder les résultats en word. Cette option n'est pas compatible avec les utilisateurs macOS. Par défaut, nous ne le feront pas (parce qu'il existe une autre option qui sera bien plus efficace pour sauvegarder les analyses que nous avons réalisées)(Figure 84).



<center>![](./fiab/fiab8.png)</center>

<small>Figure 84. Faut-il sauvegarder les items en MSword ?</small>


Notez que, comme pour les matrices de corrélations, s'il y a des valeurs manquantes, easieR vous le signalera et vous demandera de prendre une décision. 


#### Avec les lignes de commandes


La fonction qui permet de réaliser les mesures de fiabilité est l'alpha de Cronbach

- X : caractère qui correspond au nom de la ou des variable(s) sur lesquelles l'analyse doit être réalisée. ; 

- Y : caractère. Seconde variable lorsque l'analyse est une mesure d'accord (coefficient de concordance de Kendall);

- data : nom du jeu de données ; 

- choix : caractère indiquant l'analyse qui doit être réalisée. Les valeurs autorisées sont "Alpha de Cronbach", "Correlation intra-classe","Coefficient de concordance de Kendall"

- ord : caractère. Nom des variables qui doivent être considérées comme ordinales. 


- sauvegarde : logique indiquant s'il faut une  sauvegarde en fichier msword

- outlier : caractère indiquant si les analyses doivent être réalisées sur l'ensemble des données,  s'il faut réaliser l'analyse sans les valeurs influentes. Les valeurs autorisées sont : 'Donnees completes','Donnees sans valeur influente'.


- n.boot : nombre du bootstrap pour les statistiques robustes ;

- keys : caractère. Nom des variables qui ont des items inversés. 

- html : logique. Indique s'il faut une sortie html.

- imp : Procédure à suivre en cas de valeurs manquantes. Il s'agit d'un caractère à choisir parmi "rm", "mean", "median", "amelia" où "rm" signifie "remove" (on supprime), "mean" signifie qu'on remplace par la moyenne, "median" signifie qu'on remplace par la médiane et "amelia" signifie qu'on fait de l'imputation multiple sur la base de méthode Amelia. Si la valeur est "rien", cela signifie qu'on ne fait rien. 



```{r, echo=T, eval=F}
 

fiabilite(X=c('Q01','Q02','Q03','Q04','Q05','Q06','Q07','Q08','Q09','Q10','Q11',
		'Q12','Q13','Q14','Q15','Q16','Q17','Q18','Q19','Q20','Q21','Q22','Q23'),
		Y=NULL,data=diff.stat,choix='Alpha de Cronbach',
		ord=c('Q01','Q02','Q03','Q04','Q05','Q06','Q07','Q08','Q09','Q10','Q11','Q12',
		'Q13','Q14','Q15','Q16','Q17','Q18','Q19','Q20','Q21','Q22','Q23'),
			outlier='Donnees completes', keys=NULL,n.boot=0, sauvegarde=FALSE, html=F)

```




## 8.2. Interpréter les résultats


```{r, echo=F, eval=T, warning=F, message=F}
 

fia.out<-fiabilite(X=c('Q01','Q02','Q03','Q04','Q05','Q06','Q07','Q08','Q09','Q10','Q11',
		'Q12','Q13','Q14','Q15','Q16','Q17','Q18','Q19','Q20','Q21','Q22','Q23'),
		Y=NULL,data=diff.stat,choix='Alpha de Cronbach',
		ord=c('Q01','Q02','Q03','Q04','Q05','Q06','Q07','Q08','Q09','Q10','Q11','Q12',
		'Q13','Q14','Q15','Q16','Q17','Q18','Q19','Q20','Q21','Q22','Q23'),
			outlier='Donnees completes', keys=NULL,n.boot=0, sauvegarde=FALSE, html=F)

```

Dans la sortie de résultats, il y a 3 tableaux. Le premier tableau (Tableau 57) fournit la valeur de l'alpha de Cronbach brut et standardisé. L'alpha brut s'appuie sur les covariances alors que l'alpha standardisé s'appuie sur les corrélations. Il est préférable d'utiliser l'alpha standardisé qui n'est pas sensible aux différences de covariances entre les items.
Le G6 est la fiabilité Lambda 6 de Guttman (qui est un indicateur un peu meilleur que l'alpha de Cronbach). L'average_r et le median_r représente respectivement la corrélation moyenne entre les items et la corrélation médiane entre les items. 
 	
S/N est le ratio signal/bruit. Les valeurs supérieures à 1 indique un signal plus important que le bruit. 

En l'occurrence, l'alpha standardisé est de 0.796 ce qui indique une bonne fiabilité d'échelle. Le G6 a également une valeur élevée et le rapport signal/bruit est supérieur à 1, indiquant une bonne fiabilité d'échelle. 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(fia.out[[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 57. Mesures de fiabilité."
ht

```

Le second tableau (Tableau 58) est l'intervalle de confiance sur l'alpha de Cronbach. Lorsque le nombre de bootstrap est à 0, il est calculé sur la base d'une distribution normale, sinon, il s'agit d'un intervalle de confiance empirique estimé à l'aide des boostraps. En l'occurrence, notre intervalle de confiance est petit (0.78-0.80), ce qui est normal au regard de la taille d'échantillon. Cela indique surtout que notre estimation de l'alpha de Cronbach est plutôt précise.   


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(fia.out[[2]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 58. Intervalle de confiance sur l'alpha de Cronbach."
ht

```


Dans le dernier tableau (Tableau 59), les colonnes importantes sont les colonnes 'r.cor' et 'r.drop'. La colonne 'r.cor' correspond à la fiabilité des items corrigés pourle chevauchement de l'item et la fiabilité de l'échelle. La colonne 'r.drop' correspond à la corrélation des items avec l'échelle sans cet item. Ainsi, on peut voir si un item diminue potentiellement la valeur de l'alpha de Cronbach. Les autres valeurs correspondent à la valeur que prendrait les différents indices si l'item était supprimé. En l'occurence, il n'y a pas vraiment d'items qui augmenterait la faibilité. Le seul qui permettrait d'augmenter la fiabilité si on l'enlevait serait l'item 3. 


```{r echo=F, message=F, warning=F}

tableau<-as.data.frame(fia.out[[1]])
if(!is.null(dimnames(tableau)[[1]])) tableau<-data.frame(' '=dimnames(tableau)[[1]], tableau, check.names=F)
if(any(grepl('valeur.p', names(tableau)))| any(grepl('p.value', names(tableau)))) { 
                       col<-which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))
                       if(length(col)>1) {
                         is<-unique(unlist(apply(tableau[,col], 2,myf )))+1
                         tableau[,col]<-apply(tableau[,col], 2, round.ps) 
                                        }else{
                         is<-which(tableau[, which(grepl('valeur.p', names(tableau)))]<0.05)
                         is<-is+1
                       tableau[, which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))]<-round.ps(tableau[,  which(grepl('valeur.p', names(tableau))|grepl('p.value', names(tableau)))])
                                        }
                                                                                    }
                       ht <- as_hux(tableau,  add_colnames = TRUE)
                       bottom_border(ht)[1,]<-1
                       top_border(ht)[1,]<-1
                       bottom_border(ht)[dim(ht)[1],]<-1

if(any(grepl('valeur.p', names(tableau)))|any(grepl('p.value', names(tableau)))) {
                       ht<-set_text_color(ht, row = is,col =everywhere ,  value='red')
                       } 
if(any(class(table)=='p.value')){
     for(j in 1:ncol(tableau)){
         ht<-set_text_color(ht, row = is,col =everywhere ,  value='red', byrow = T)

     }}
caption(ht) <-  "Tableau 59. Valeur de la corrélation entre les items et l'échelle si l'item d'une ligne donnée est supprimé."
ht

```


## 8.3. Exercices {.tabset .tabset-fade .tabset-pills}

### Enoncés

Une université veut recruter un nouvel enseignant pour les cours de statistiques. Les responsables du recrutement décident de développer un nouvel outil qu’ils vont faire passer aux candidats pour le poste. Vérifiez la fiabilité de cet outil 
 (cet exemple est inspiré de Field et al., 2013, p. 809). Voici les questions :

- Je me suis une fois réveillé dans un lopin de légumes en tenant un navet que j’avais accidentellement déraciné en pensant avoir trouvé la plus grande racine de Roy					

- Si j’avais un gros flingue, je tirerais sur tous les étudiants à qui je dois enseigner. 					

- Je mémorise les valeurs des probabilités pour la distribution des F					

- Je vénère un sanctuaire dédié à Pearson					

- Je vis encore avec ma mère et mon hygiène corporelle laisse à désirer					

- La douleur de mon œsophage en train de brûler après avoir avalé une bouteille d’eau de javel ne serait rien en comparaison de devoir enseigner aux autres.					

- Aider les autres à comprendre ce qu’est la somme des carrés est un sentiment génial					

- J’aime vérifier les conditions d’application					

- Je calcule une ANOVA à trois facteurs tous les matins avant de sortir du lit					

- Je pourrais passer toute ma journée à expliquer des statistiques aux autres					

- J’aime qu’on me dise que j’ai aidé quelqu’un à comprendre ce qu’était une rotation factorielle					

- Les gens s’endorment dès que je commence à parler					

- Monter des expériences, c’est amusant					

- Je préfère réfléchir aux meilleures variables dépendantes plutôt que d’aller boire un verre					

- Je souille mon pantalon d’excitation à la simple mention d’analyse factorielle					

- Choisir entre une mesure INTER et une mesure INTRA me ravit					

- Quand je m’assois dans un parc, je contemple les gens en me demandant comment je pourrais les utiliser comme participants dans mes prochaines expériences					

- Etre face à 300 personnes ne me fait en aucune manière perdre le contrôle de mon intestin					

- J’aime aider les étudiants					


- Transmettre des connaissances est le plus beau cadeau qu’on puisse faire à un individu					

- Penser aux corrections de Bonferroni émoustille mon aine					

- je frissonne quand je pense au design de ma prochaine expérience					

- Je passe souvent mon temps libre à parler aux pigeons… et même eux meurent d’ennuis					

- J’ai essayé de construire une machine à voyager dans le temps pour retourner dans les années 1930 et suivre Fisher à genoux en léchant le sol sur lequel il venait juste de passer					

- J’adore enseigner					

- Je passe beaucoup de temps à aider les étudiants					

- J’aime enseigner parce que les étudiants doivent faire semblant de m’aimer s’ils ne veulent pas avoir de mauvaises notes					

- Mon chat est mon unique ami				

Les données sont dans la feuille 'recrut.stat'. Utilisez la fonction en ligne de commande. Utilisez également la fonction <code>dput</code> et <code>names</code> pour indiquer plus rapidement le nom des variables. Faites attention que des valeurs manquantes ont été signalées lors de l'importation des données. Prenez cette information en compte quand vous utiliserez la fonction. 


```{r, echo=F}

import(file='Exercices.Bruxelles.xlsx',
	dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
	type='Fichier Excel',dec='.',sep=';',na.strings='NA',sheet='recrut.stat',name='recrut.stat')

datatable(battre, caption="Tableau 55. Jeu de données 'recrut.stat'")

```

### Solutions 




```{r, echo=F}

fiabilite(X=c("q1", "q2", "q3", "q4", "q5", "q6", "q7", "q8", "q9", "q10", 
		"q11", "q12", "q13", "q14", "q15", "q16", "q17", "q18", "q19", 
		"q20", "q21", "q22", "q23", "q24", "q25", "q26", "q27", "q28"),
		Y=NULL,data=recrut.stat,choix='Alpha de Cronbach',
		ord=c("q1", "q2", "q3", "q4", "q5", "q6", "q7", "q8", "q9", "q10", 
			"q11", "q12", "q13", "q14", "q15", "q16", "q17", "q18", "q19", 
			"q20", "q21", "q22", "q23", "q24", "q25", "q26", "q27", "q28"),
			outlier='Donnees completes', keys=NULL,n.boot=0, sauvegarde=FALSE, imp="rien")

```





# 9. Les analyses de variance

## 9.1. Les analyses de variance simple

Pour illustrer l'utilisation de l'anova dans easieR, nous nous appuierons sur une étude qui visait à identifier comment les intentions sexuelles était interprétées par les personnes impliquées dans une conversation.La manière dont les hommes perçoivent les intentions sexuelles d’une femme est systématiquement surestimé par rapport à ce que rapporte les femmes de de leurs propres intentions sexuelles. Cependant, comme les intentions sexuelles des femmes ne peuvent pas être directement mesurées, une hypothèse alternative est que les femmes minimisent leurs intentions sexuelles. Dans ce cas, l'évaluation des hommes serait généralement plutôt précise. Pour tenter de déterminer l'hypothèse la plus probable entre les deux, des chercheurs ont demandé à des femmes impliquées dans une interaction avec un homme (femme_auto), à l’homme impliqué dans l’interaction (homme) et à une femme observant l’interaction (femme_hétéro) d'évaluer les intentions sexuelles de la femme impliquée dans l’interaction sur une échelle qui fournit un score composite.

Les données sont dans "sex.intentions" (inspiré de Perilloux & Kurzban, 2014). 
Les données sont présentées dans le Tableau 56


```{r, echo=F}

import(file='Exercices.Bruxelles.xlsx',
	dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
	type='Fichier Excel',dec='.',sep=';',na.strings='NA',sheet='sex.intentions',name='sex.intentions')

datatable(sex.intentions, caption="Tableau 56. Jeu de données 'intentions sexuelles'")

```

### 9.1.1. Les boîtes de dialogue 

On peut directement accéder aux boîtes de dialogue de l'analyse de variance en utilisant la fonction <code>ez.anova</code>. 


La première boîte de dialogue consiste à choisir le jeu de données (Figure 85).

<center>![](./anova/F1I/f1i1.png)</center>
<small>Figure 85. Choix du jeu de données</small>

L'étape suivante consiste à choisir le type de plan que nous voulons réaliser. Pour une anova à facteurs indépendants, on utilise uniquement "Groupes indépendants". On peut choisir une ou plusieurs options en maintenant la touche ctrl enfoncée (pomme pour les Mac Users).En l'occurrence, il s'agit d'un plan à groupes indépendants (Figure 86). 


<center>![](./anova/F1I/f1i2.png)</center>

<small>Figure 86. Type de plan pour l'anova. En l'occurrence, il s'agit d'un plan à groupes indépendants.</small>

Il faut ensuite choisir **la OU les** variables à groupes indépendants. Dans notre exemple, nous n'avons qu'une seule variable, qui est groupe (Figure 87).  

<center>![](./anova/F1I/f1i3.png)</center>

<small>Figure 87. Choix de la ou des variables à groupes indépenants.</small>

ON choisit ensuite la variable dépendante, 'intentions' dans notre cas (Figure 88). 


<center>![](./anova/F1I/f1i4.png)</center>

<small>Figure 88. Choix de la variable dépendante.</small>

Il faut ensuite déterminer quelles analyses sont souhaitées (Figure 89). Le test paramétrique est l'anova, le non paramétrique est le test de Krauskal-Wallis (à la condition de n'avoir qu'une variable indépendante), les tests robustes vont proposer des boostraps et des analyses sur des indicateurs plus robustes (comme les médianes ou les moyennes tronquées). Enfin, les facteurs bayesiens permettent d'obtenir les analyses sous un angle bayesien. En l'occurrence, nous choisirons les statistiques paramétrriques et non paramétriques (Figure 88). Il faut noter que l'option "test non paramétriques" n'est pas proposé s'il y a plus qu'une variable indépedante. 


<center>![](./anova/F1I/f1i5.png)</center>

<small>Figure 89. Type d'analyse qui devra être réalisé entre les statistiques paramétriques, non paramétrique, robustes et les facteurs bayesiens</small>

On continue en déterminant si on souhaite que l'analyse soit réalisée sur l'échantillon complet, s'il faut identifier la présence de valeurs influentes et/ou si on veut l'analyse sur l'échantilon dont on a supprimé les valeurs influentes. Par défaut, les valeurs influentes sont identifiées et supprimées (Figure 90).

La plupart du temps, il est préférable d'analyser les données sans les valeurs influentes. Néanmoins, il y a des exceptions : par exemple, quand on travaille avec une population pathologique, il est normal de s'attendre à des valeurs influentes. Par ailleurs, certains chercheurs préférent laisser toutes les observations ou utiliser une autre alternative (e.g., les tests non paramétriques) que de supprimer les valeurs influentes. Une attitude minimale est d'aller identifier s'il y a des valeurs influentes. Par défaut, c'est le test de Grubbs qui est utilisé pour identifier la présence des valeurs influentes. Entre l'analyse "sans les valeurs influentes" et "avec les valeurs influentes", il est préférable de choisir "sans" dans la majorité des cas. Néanmoins, je trouve toujours intéressant de regarder si l'interprétation change lorsque les valeurs influentes sont présentes ou non, peu importe le sens. En effet, cela donne une appréciation de la robustesse de l'effet, et permet de s'assurer que la présence/absence de l'effet ne dépend pas que de quelques observations, c'est pourquoi il est préférable de laisser tout coché dans la Figure 90

<center>![](./anova/F1I/f1i6.png)</center>

<small>Figure 90. Sur quel échantillon les analyses doivent-elles être réalisées ? L'échantillon complet ou sans les valeurs influentes. Faut-il identifier ces valeurs influentes.</small>

Il existe plusieurs manière de calculer les tailles d'effet dans une anova. La manière la plus robuste est l'omega carré (qui est forcément fourni). Cependant, les logiciels commerciaux ne fournissent la plupart du temps que le $\eta^2$ partiel. Cette mesure est la moins fiable. Il existe une alternative un peu meilleure qui est le $\eta^2$ généralisé. Ici 'ges' renvoie au $\eta^2$ généralisé et 'pes' renvoie au $\eta^2$ partiel (Figure 91).


<center>![](./anova/F1I/f1i7.png)</center>

<small>Figure 91. Type de taille d'effet à calculer.</small>

Une des particularités de l'analyse de variance est que la manière de réaliser l'analyse dépend de nos hypothèses. Si l'hypothèse principale est une hypoothèse d'interaction, ce qui est le plus souvent le cas, on doit réaliser une ANOVA avec une somme de carré de type 3. Les puristes considèrent même que les effets principaux ne doivent pas être intérprétés dans ce cas. Des auteurs comme Howell (2009) et Field (2012) sont moins rigides sur la question, et considèrent qu'il est possible d'interpréter les résultats des effets principaux si on est prudent et qu'on s'appuie sur le graphique pour ne pas être induit en erreur. Lorsque l'hypothèse principale porte sur les effets principaux, on utilisera une somme des carrés de type 2 (Figure 92). En l'occurence, comme il n'y a qu'un seul facteur, il ne peut pas y avoir d'interaction, le choix de l'un ou l'autre aboutit aux mêmes résultats. 

<center>![](./anova/F1I/f1i8.png)</center>

<small>Figure 92. Choix de la somme des carrés à devoir calculer. Avec un seul facteur, ce choix est indifférent.</small>


La boîte de dialogue suivante permet de sauvegarder les résultats en word. Cette option n'est pas compatible avec les utilisateurs macOS. Par défaut, nous ne le feront pas (parce qu'il existe une autre option qui sera bien plus efficace pour sauvegarder les analyses que nous avons réalisées)(Figure 93).

<center>![](./anova/F1I/f1i9.png)</center>

<small>Figure 93 Faut-il enregistrer les résultats en MSword. En l'occurence, le choix est négatif.</small>


L'anova permet de savoir qu'il y a au moins une moyenne qui se distingue des autres. Cependant, on ne sait pas laquelle. Pour identifier entre quelles modalités, il existe des différences, on doit réaliser des contrastes. Ces contrastes peuvent être choisi a priori, peuvent être les comparaisons de toutes les moyennes 2 à 2 (il faudra dans ce cas corriger la probabilité, comme dans une matrice de corrélation) ou ne pas faire de contraste (Figure 94). En l'occurrence, nous avons des hypoothèses sur les différences attendues. Nous choisissons donc des contrastes a priori. 

<center>![](./anova/F1I/f1i10.png)</center>

<small>Figure 94. Choix du type de contrastes .</small>

Dans la console va alors apparaître une liste de contrastes préprogrammés qui est utile quand on est sûr de l'ordre des modalités pour les facteurs. En effet, les modalités des facteurs sont organisées par défaut en ordre alphabétique. On peut donc utiliser des contrastes préprogrammés dans easieR (Figure 95). 

<center>![](./anova/F1I/f1i11.png)</center>

<small>Figure 95. Coefficients appliqués aux contrastes préprogrammés.</small>

Cependant, quand on n'a pas de certitude sur l'organisation des modalités, on peut spécifier soi-même les contrastes (Figure 96). 

<center>![](./anova/F1I/f1i12.png)</center>

<small>Figure 96. Liste des contrastes préprogrammés et possibilité de choisir ses propres contrastes.</small>

**Attention** Quand on réalise ses contrastes soi-même, il est nécessaire qu'ils respectent l'orthogonalité. Il existe une autre option dans easieR où l'orthogonalité n'est pas obligatoire. Cependant, cette fonction est temporairement inutilisable. 

Dans la Figure 97, vous pouvez voir les contrastes que nous avons choisi d'entrer. Nos contrastes respectent l'orhogonalité puisque la somme de chaque colonne est égale à 0 et que le somme des produits des deux colonnes vaut également 0 (i.e., (-1*1)+(-1*-1)+(2*0)=0)

<center>![](./anova/F1I/f1i13.png)</center>

<small>Figure 97. Coefficients pour les deux contrastes qui respectent l'orthogonalité.</small>




### 9.1.2. Les lignes de commande


La fonction <code>ez.anova()</code> permet de réaliser les analyses de variances en ligne de commande. Les arguments de cette fonction sont :

- data : nom du jeu de données ; 

- DV : caractère qui correspond au nom de la variable ou des variables dépendantes

- between : caractère. Nom de la ou des variables à groupes indépendants. Cette variable doit être qualitative.

- within : caractère. Nom de la ou des variables en mesure répétée (format long). Cette variable doit être qualitative. 

- cov : caractère. Nom de la ou des covariables. La covariable doit être numérique.

- id : caractère. Nom de la variable identifiant les participants. Cette variable doit être qualitative. Quand on passe par les boîte de dialogue, une nouvelle variable est créée (IDeasy) et easieR infère que c'est la variable identifiant les participants.

- param : caractère indiquant quelles analyses doivent être faites. Les valeurs autorisées sont : 'Test parametrique','Test non parametrique','Test robustes - impliquant des bootstraps','Facteurs bayesiens', ou leur version abrégée 'param', 'non param', 'robustes', 'Bayes'

- outlier : caractère indiquant si les analyses doivent être réalisées sur l'ensemble des données, s'il faut identifier les valeurs influentes et s'il faut réaliser l'analyse sans les valeurs influentes. Les valeurs autorisées sont : 'Donnees completes','Identification des valeurs influentes','Donnees sans valeur influente'

- SumS : Type de sommes de carré à devoir calculer. Caractère. Les valeurs autorisées sont "2" et "3".  

- ES : type de taille d'effet à devoir calculer. Les valeurs autorisées sont 'pes' pour le $\eta^2$ partiel et 'ges' pour le généralisé.

- save : logique indiquant s'il faut une  sauvegarde en fichier msword

- html : logique. Indique s'il faut une sortie html. 

- contrasts : type de contrastes. Les valeurs autorisées sont 'none' pour aucun contraste, 'pairwise' pour les comparaisons 2 à 2 ou une liste avec le nom des variables indépendants comme nom de chaque élément et une matrice de coefficients comme valeurs (voir exemple). 

- p.adjust : Si les contrastes sont des comparaisons 2 à 2, quel est la correctoin de la probabilité qu'il faut appliquer ? Les valeurs autorisées sont "holm", "hochberg", "hommel", "bonferroni", "fdr","tukey","scheffe", "sidak","dunnettx","mvt" ,"none" 

- n.boot : nombre du bootstrap pour les statistiques robustes et pour le posterior des facteurs bayesiens 

- info : il s'agit d'un logique qui indique si des messages d'informations doivent être affichés dans la console pour expliquer à quoi correspond les boîtes de dialogue. Par défaut, la valeur est TRUE. 

- rscaleFixed : valeur numérique correspondant au prior des facteurs bayesiens pour les effets fixes (ici le groupe). si 'Bayes' ou 'Facteurs bayesiens'est sélectionné pour 'param' .  

- rscaleRandom : il s'agit du prior sur le facteur aléatoire (ici les participants) si 'Bayes' ou 'Facteurs bayesiens'est sélectionné pour 'param' . 




```{r, echo=T, eval=F, warning=F, message=F}

ez.anova(data=sex.intentions, DV='Intentions', between =c('groupe'), within =NULL, 
		cov=NULL,id ='IDeasy', param =c('Modele parametrique','Modele non parametrique'), 
		outlier= c('Donnees completes','Identification des valeurs influentes',
		'Donnees sans valeur influente'), ES ='ges', SumS= '3', save =FALSE, 
		html =F, contrasts =list(groupe=matrix(c(-1,-1,2,1,-1,0), ncol=2)), 
		p.adjust = 'none', n.boot = 1000,rscaleFixed = 0.5, rscaleRandom = 1)
```



## 9.2. Les analyses de variance à mesure répétée avec un facteur


Stefaniak et al. (2008) se sont intéressés aux mécanismes d’apprentissage implicite. Plus précisément, ils se sont intéressés au débat qui existe quant à la possibilité d’utiliser des paradigmes d’apprentissage implicite en situation expérimentale. En effet, des personnes comme Shanks (2005) vont considérer que l’apprentissage en situation expérimentale est toujours influencé par des processus explicites. L’hypothèse de Stefaniak et al. (2008) est que, dans certaines conditions, même en fournissant de manière explicite  l’information pertinente pour l’apprentissage, il n’est pas possible d’utiliser les processus explicites d’apprentissage. Pour ce faire, ils ont utilisé la tâche de temps de réaction sériel (qui est une tâche d’apprentissage implicite). Dans cette tâche, on demande aux participants de répondre le plus rapidement et le plus précisément possible aux stimuli qui apparaissent à différentes localisation sur l’écran. Les stimuli n’apparaissent pas de manière aléatoire mais suivent, à l’insu des participants, une séquence. La tâche est composée de 13 blocs. Les blocs 1 à 12 sont des blocs d’apprentissage. Le bloc 13 est un bloc de transfert dans lequel une autre séquence que la séquence d’apprentissage est utilisée. L’apprentissage se manifeste par des temps de réaction plus courts pour le bloc 12 que pour le bloc 1, ainsi que par des temps de réaction plus courts pour le bloc 12 que pour le bloc 13.  Le bloc 1 et 13 ne doivent pas se distinguer pour exclure la possibilité d'un effet d'habituation à la tâche plutôt que l'effet d'un apprentissage spécifique. 

Nous allons tester si un apprentissage a eu lieu (indépendamment des groupes)

Les données sont issues de Stefaniak et al. (2008). 

Les données sont présentées dans le Tableau 58


```{r, echo=F}

import(file='Exercices.Bruxelles.xlsx',
       dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
       type='Fichier Excel',
       dec='.',
       sep=';',
       na.strings='NA',
       sheet='AI',
       name='AI')

datatable(AI, caption="Tableau 58. Jeu de données 'AI'")

```


### 9.2.1. En boîte de dialogue 


Fondamentalement, la procédure pour réaliser une analyse variance à mesure répétée est la même que celle pour une analyse de variance à groupes indépendants. La seule différence va être dans le choix des variables. 


La première étape est choisir 'mesures répétées' au lieu de 'groupes indépendants'(Figure 98).  

<center>![](./anova/F1R/MR1.png)</center>

<small>Figure 98. Choix d'une analyse de variance à mesure répétée uniquement.</small>

En effet, easieR peut traiter les facteurs en mesure répétées qui sont dans un format large ou dans un format long. Pour rappel, le format large est le format où chaque ligne représente un participant et où les modalités de la variables à mesure répétées sont dans des colonnes séparées. 


Un jeu de données est habituellement présenté de la manière suivante : une colonne par variable et une ligne par participant. Les choses se compliquent lorsque nous avons des variables en mesure répétées, c'est-à-dire des variables pour lesquelles on réalise la même mesure à plusieurs moments chez le même individu. Deux modes de présentation existent : le format large et le format long. 

Dans le format large, pour les facteurs en mesure répétées, une colonne représente une modalité de la variable à mesure répétée (ou une combinaison de modalités s'il y a plusieurs variables). Pour illustrer ce phénomène (voir Tableau 59 issu de la feuille choc). Ce jeu de données correspond à l'énoncé suivant : suite à la mécompréhension d'un article de Milgram sur la soumission à l'autorité, nous avons voulu savoir si des chocs électriques fonctionnaient pour l'apprentissage d'habiletés, comme le lancer de fléchettes. Nous avons donc soumis des participants à trois séances d'apprentissage de lancers de fléchettes, séparées en une session avec choc, et une session sans choc. Par ailleurs, la variable groupe indique les habiletés auto-déclarées des participants sur une tâche de lancer de fléchettes. Ces données sont présentées dans le Tableau 59.


```{r, echo=F}
import(file='Exercices.Bruxelles.xlsx',
	dir='C:/Users/stefan01/OneDrive - URCA/reims/recherches/Présentation easieR - Bruxelles 2021',
	type='Fichier Excel',dec='.',sep=';',na.strings='NA',sheet='choc',name='choc')


datatable(choc, caption="Table 59. Exemple d'une présentation en format large")

```




Dans l’exemple présenté dans le Tableau 59, la première colonne représente le groupe et les colonnes 2 à 7 représente la combinaison entre le numéro de la séance (séance 1, séance 2, séance 3) et le type de session (avec choc vs. sans choc). Les valeurs présentées en-dessous de ces entêtes représentent la distance par rapport au centre de la cible. Au final, le nom de la colonne contient les informations de 2 variables (la séance et la présence du choc) fournissant les modalités de la séance et de la présene du choc tandis que les informations présentées sous la colonne ne correspondent pas à ce qui est annoncé dans l'entête de colonne. Cette manière de présenter les données est donc illogiques et crée de l’ambiguïté. 

**Note importante** : dans ce jeu de données, une erreur peut être observée dans la manière dont le jeu de données a été construit : il n'y a aucun identifiant des participants. Si, pour des raisons légales, il est strictement interdit d'avoir le nom des participants (du moins s'ils sont humains), il faut un code qui permet de les identifier afin de s'assurer que les observations sont indépendantes. 


Pour faire face à ce problème, on peut utiliser un format long. Dans le format long, une colonne représente une variable. Les mesures répétées sont identifiables par le fait que l'identifiant des individus (la variable aléatoire) va être répété. Cette présentation est fournie dans le Tableau 60. 

```{r, echo=F, warning=F}
library(easieR)
choc.long<-ez.reshape(data=choc, 
           varying =list(c('sean1sans_choc','sean2sans_choc',
          'sean3sans_choc','sean1choc','sean2choc','sean3choc')),
           v.names =c('distance'),idvar =c('groupe'),
          IV.names=list('choc','seance'),
          IV.levels=list( c('sans','avec'), c('s1','s2','s3')))
datatable(choc.long, caption="Tableau 60. Exemple d'une présentation en format long")

```




L'étape suivante consiste à choisir les modalités de la variable à mesure répétée si la présentation est en format large ou la variable indépendante si les données sont présentées en format long.  Si on utilise un format long, la variable dépendante doit être numérique et la variable indépendante doit être qualitative avec deux ou plusieurs modalités. Il est donc nécessaire de s'en être assuré lors de l'importation des données. 



**Note** Si on avait eu une présentation en format long, il aurait fallu une variable dépendante et indépendante de manière tout à fait identique à ce qu'on a dans l'anova à groupe indépendant mais on choisirait en plus l'identifiant des participants pour que easieR puisse appariés les valeurs correspondant à un même individu. 



<center>![](./anova/F1R/MR2.png)</center>

<small>Figure 99. Choix de modalités en mesure répétées pour une présentation en format large.</small>


Puisque nous avions un format large, easieR va transformer les données en un format long. 

Il faut donc donner un nom à la variable indépendante. Ces noms doivent non seulement être explicites et serviront pour les graphiques, il est donc opportun de choisir des noms qui pourront) servir dans la graphique final (Figure 74). En l'occurrence, la variable indépendante peut s'appeler 'BLOC' (Figure 100). 

<center>![](./anova/F1R/MR3.png)</center>

<small>Figure 100. Nom de la variable indépendante en mesure répétée.</small>

On peut également réaliser l'analyse avec une présentation en format long. Dans ce cas, on doit choisir le facteur en mesure répétée, l'identifiant des partiicipants et la variable dépendante ('value' si on est passé par easieR auparavant) (Figure 101).


<center>![](./anova/F1R/MR4.png)![](./anova/F1R/MR5.png)![](./anova/F1R/MR6.png)</center>

<small>Figure 101. Choix des variables indépendantes, identifiant le participant et dépendante quand la présentaton est en format long.</small>




### 9.2.2. Les lignes de commande

Les explications étant les mêmes que celles présentées pour l'anova à groupe indépendants, nous renvoyons le lecteur vers cette section. 
















